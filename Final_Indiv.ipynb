{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7-cgG5iK8DHq",
        "outputId": "004425a6-b828-4a94-f1bb-21dbb11dc519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting catboost==1.2.7\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (3.10.0)\n",
            "Collecting numpy<2.0,>=1.16.0 (from catboost==1.2.7)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (1.14.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost==1.2.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost==1.2.7) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost==1.2.7) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost==1.2.7) (9.1.2)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, catboost\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed catboost-1.2.7 numpy-1.26.4\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit --quiet\n",
        "!pip install streamlit pyngrok --quiet\n",
        "!pip install catboost==1.2.7\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Voting* Regressor"
      ],
      "metadata": {
        "id": "MxvadfCH8YL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "# ✅ Load the data\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/육사 부식 잔반 최적화/current_processed_menu_data.xlsx\")\n",
        "\n",
        "# ✅ Select relevant columns and drop NaN values\n",
        "data_subset = data[['Meal Type', 'Menu', 'Dessert', 'Event', 'leftovers']].dropna()\n",
        "\n",
        "# ✅ Convert categorical columns to string\n",
        "data_subset['Meal Type'] = data_subset['Meal Type'].astype(str)\n",
        "data_subset['Dessert'] = data_subset['Dessert'].astype(str)\n",
        "\n",
        "# ✅ Convert multi-label categorical columns to lists\n",
        "data_subset['Menu'] = data_subset['Menu'].astype(str).apply(lambda x: x.split(','))\n",
        "data_subset['Event'] = data_subset['Event'].astype(str).apply(lambda x: x.split(','))\n",
        "\n",
        "# ✅ Compute historical average leftovers per menu item\n",
        "menu_avg_leftovers = data_subset.explode('Menu').groupby('Menu')['leftovers'].mean().to_dict()\n",
        "data_subset['Menu Avg Leftovers'] = data_subset['Menu'].apply(\n",
        "    lambda items: np.mean([menu_avg_leftovers.get(item, 0) for item in items])\n",
        ")\n",
        "\n",
        "# ✅ One-hot encode multi-label categorical variables\n",
        "mlb_menu = MultiLabelBinarizer()\n",
        "mlb_event = MultiLabelBinarizer()\n",
        "menu_encoded = pd.DataFrame(mlb_menu.fit_transform(data_subset['Menu']), columns=mlb_menu.classes_)\n",
        "event_encoded = pd.DataFrame(mlb_event.fit_transform(data_subset['Event']), columns=mlb_event.classes_)\n",
        "\n",
        "# ✅ Concatenate encoded menu & event features with other categorical features\n",
        "data_encoded = pd.concat([data_subset.drop(columns=['Menu', 'Event']), menu_encoded, event_encoded], axis=1)\n",
        "\n",
        "# ✅ Define categorical columns\n",
        "categorical_features = ['Meal Type', 'Dessert']\n",
        "\n",
        "# ✅ Preprocessing pipeline: One-hot encode categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
        "        ('scale', StandardScaler(), ['Menu Avg Leftovers'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# ✅ Apply preprocessing\n",
        "X = preprocessor.fit_transform(data_encoded.drop(columns=['leftovers']))\n",
        "\n",
        "# ✅ Convert processed X into a DataFrame\n",
        "X = pd.DataFrame(X, columns=preprocessor.get_feature_names_out())\n",
        "\n",
        "# ✅ Define target variable (log-transformed leftovers)\n",
        "y_total = np.log1p(data_encoded[['leftovers']])\n",
        "\n",
        "# ✅ Train-test split for total leftovers prediction\n",
        "X_train, X_test, y_train_total, y_test_total = train_test_split(X, y_total, test_size=0.2, random_state=42)\n",
        "\n",
        "# ✅ Train a new Voting Regressor\n",
        "voting_regressor = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('XGBoost', XGBRegressor(n_estimators=300, learning_rate=0.01, max_depth=4, objective=\"reg:squarederror\", random_state=42)),\n",
        "        ('CatBoost', CatBoostRegressor(iterations=500, learning_rate=0.01, depth=6, verbose=0, random_state=42)),\n",
        "        ('RandomForest', RandomForestRegressor(n_estimators=300, max_depth=20, min_samples_leaf=5, random_state=42))\n",
        "    ]\n",
        ")\n",
        "\n",
        "voting_regressor.fit(X_train, y_train_total.values.ravel())\n",
        "y_pred_total = np.expm1(voting_regressor.predict(X_test))\n",
        "\n",
        "# ✅ Compute R² score for Voting Regressor\n",
        "regressor_r2 = r2_score(y_test_total, np.log1p(y_pred_total))\n",
        "print(f\"Voting Regressor R²: {regressor_r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0PZvUaq8VtN",
        "outputId": "f115eac0-8d4d-4a8a-b126-c9a1b1d4f75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Regressor R²: 0.6179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained Voting Regressor\n",
        "joblib.dump(voting_regressor, \"/content/drive/MyDrive/육사 부식 잔반 최적화/voting_regressor.pkl\")\n",
        "\n",
        "# Save the preprocessor\n",
        "joblib.dump(preprocessor, \"/content/drive/MyDrive/육사 부식 잔반 최적화/preprocessor.pkl\")\n",
        "\n",
        "# Save MultiLabelBinarizers\n",
        "joblib.dump(mlb_menu, \"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_menu.pkl\")\n",
        "joblib.dump(mlb_event, \"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_event.pkl\")\n",
        "\n",
        "print(\"All components saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZocaeJ68ayo",
        "outputId": "40c001c1-1dee-49c6-c633-56db71ded3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All components saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 🔄 Reload trained components\n",
        "voting_regressor = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/voting_regressor.pkl\")\n",
        "preprocessor = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/preprocessor.pkl\")\n",
        "mlb_menu = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_event.pkl\")\n",
        "model = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/voting_regressor.pkl\")\n"
      ],
      "metadata": {
        "id": "04EtLFox8cQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix Factorization + NNLS"
      ],
      "metadata": {
        "id": "FSpLpqT38jZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 고유 음식 종류 개수 출력\n",
        "unique_items = set(item.strip() for sublist in data_subset['Menu'] for item in sublist)\n",
        "print(f\"총 고유 음식 종류 수: {len(unique_items)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZIdRf-r8hsQ",
        "outputId": "88e6c379-4976-4be2-cc75-63a94aa37659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 고유 음식 종류 수: 694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import nnls\n",
        "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import joblib\n",
        "\n",
        "\n",
        "def train_matrix_factorization(data_subset, mlb_menu, n_factors=700):  #n_factors 줄이면 더 그럴싸해보임\n",
        "    \"\"\"Train NMF with robust normalization to avoid division by zero\"\"\"\n",
        "    historical_menu_encoded = mlb_menu.transform(data_subset['Menu'])\n",
        "\n",
        "    # Ensure valid factorization dimensions\n",
        "    n_samples, n_features = historical_menu_encoded.shape\n",
        "    n_factors = min(n_factors, n_samples, n_features)\n",
        "\n",
        "    nmf = NMF(\n",
        "        n_components=n_factors,\n",
        "        init='nndsvda',\n",
        "        solver='mu',\n",
        "        beta_loss='kullback-leibler',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    W = nmf.fit_transform(historical_menu_encoded)\n",
        "    H = nmf.components_\n",
        "\n",
        "    # Stable normalization with condition number check\n",
        "    norms = np.linalg.norm(H, axis=0, keepdims=True)\n",
        "    H_normalized = H / np.where(norms < 1e-10, 1e-10, norms)\n",
        "\n",
        "    return nmf, H_normalized\n",
        "\n",
        "# ================== NNLS Distribution ==================\n",
        "def distribute_leftovers(total_leftovers, menu_encoded_values, H):\n",
        "    \"\"\"Distribute totals using NNLS with item interactions\"\"\"\n",
        "    individual_preds = []\n",
        "\n",
        "    for total, menu_vec in zip(total_leftovers, menu_encoded_values):\n",
        "        active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "        if not active_idx.size:\n",
        "            individual_preds.append(np.zeros_like(menu_vec))\n",
        "            continue\n",
        "\n",
        "        # Use latent factors for active items\n",
        "        H_sub = H[:, active_idx]\n",
        "        A = H_sub.T @ H_sub + 1e-6 * np.eye(H_sub.shape[1])  # Regularization\n",
        "        b = H_sub.T @ np.ones(H_sub.shape[0]) * total\n",
        "\n",
        "        weights, _ = nnls(A, b)\n",
        "\n",
        "        # Force sum to match total leftovers exactly\n",
        "        weights /= weights.sum() + 1e-10  # Normalize weights to sum to 1\n",
        "        weights *= total                 # Scale weights to match total\n",
        "\n",
        "        full_weights = np.zeros_like(menu_vec)\n",
        "        full_weights[active_idx] = weights\n",
        "\n",
        "        individual_preds.append(full_weights)\n",
        "\n",
        "    return np.array(individual_preds)\n",
        "\n",
        "def predict_new_meals(new_meals):\n",
        "\n",
        "    def predict_total_leftovers(new_meals):\n",
        "      \"\"\"\n",
        "      Predicts total leftovers for new meal combinations using the Voting Regressor.\n",
        "      Handles unseen combinations of known menu items.\n",
        "      \"\"\"\n",
        "      # Preprocess new data to match training structure\n",
        "      processed = new_meals.copy()\n",
        "      processed[\"Menu\"] = processed[\"Menu\"].str.split(\",\")\n",
        "      processed[\"Event\"] = processed[\"Event\"].str.split(\",\")\n",
        "\n",
        "      # Filter to known menu/event items from training data\n",
        "      valid_menu = set(mlb_menu.classes_)\n",
        "      valid_events = set(mlb_event.classes_)\n",
        "      processed[\"Menu\"] = processed[\"Menu\"].apply(\n",
        "          lambda x: [item.strip() for item in x if item.strip() in valid_menu]\n",
        "      )\n",
        "      processed[\"Event\"] = processed[\"Event\"].apply(\n",
        "          lambda x: [item.strip() for item in x if item.strip() in valid_events]\n",
        "      )\n",
        "\n",
        "      # Compute \"Menu Avg Leftovers\" using historical training averages\n",
        "      processed[\"Menu Avg Leftovers\"] = processed[\"Menu\"].apply(\n",
        "          lambda items: np.mean([menu_avg_leftovers.get(item, 0) for item in items])\n",
        "      )\n",
        "\n",
        "      # Encode menu and event with MultiLabelBinarizer (ensure all training columns exist)\n",
        "      menu_encoded = pd.DataFrame(\n",
        "          mlb_menu.transform(processed[\"Menu\"]),\n",
        "          columns=mlb_menu.classes_,\n",
        "          index=processed.index\n",
        "      ).reindex(columns=mlb_menu.classes_, fill_value=0)  # Force all training columns\n",
        "\n",
        "      event_encoded = pd.DataFrame(\n",
        "          mlb_event.transform(processed[\"Event\"]),\n",
        "          columns=mlb_event.classes_,\n",
        "          index=processed.index\n",
        "      ).reindex(columns=mlb_event.classes_, fill_value=0)  # Force all training columns\n",
        "\n",
        "      # Prepare raw features for ColumnTransformer\n",
        "      features = pd.concat([\n",
        "          processed[[\"Meal Type\", \"Dessert\", \"Menu Avg Leftovers\"]],\n",
        "          menu_encoded,\n",
        "          event_encoded\n",
        "      ], axis=1)\n",
        "\n",
        "      # Apply the preprocessor (includes OneHotEncoder for Meal Type/Dessert)\n",
        "      aligned_features = preprocessor.transform(features)\n",
        "\n",
        "      # Predict totals\n",
        "      totals = np.expm1(voting_regressor.predict(aligned_features))\n",
        "\n",
        "      return totals\n",
        "\n",
        "    \"\"\"Predict total and distribute to items without historical averages\"\"\"\n",
        "    # Load components\n",
        "    model = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/voting_regressor.pkl\")\n",
        "    preprocessor = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/preprocessor.pkl\")\n",
        "    mlb_menu = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_menu.pkl\")\n",
        "    mlb_event = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_event.pkl\")\n",
        "    # Preprocessing\n",
        "    processed = new_meals.copy()\n",
        "    processed[\"Menu\"] = processed[\"Menu\"].str.split(\",\")\n",
        "    processed[\"Event\"] = processed[\"Event\"].str.split(\",\")\n",
        "\n",
        "    # Keep all input items (even unseen ones)\n",
        "    all_items = list(set(item for sublist in processed[\"Menu\"] for item in sublist))\n",
        "\n",
        "    # Encode features\n",
        "    menu_encoded = pd.DataFrame(\n",
        "        mlb_menu.transform(processed[\"Menu\"]),\n",
        "        columns=mlb_menu.classes_,\n",
        "        index=processed.index\n",
        "    ).reindex(columns=all_items, fill_value=0)\n",
        "\n",
        "    event_encoded = pd.DataFrame(\n",
        "        mlb_event.transform(processed[\"Event\"]),\n",
        "        columns=mlb_event.classes_,\n",
        "        index=processed.index\n",
        "    )\n",
        "\n",
        "    # Prepare features\n",
        "    features = pd.concat([\n",
        "        processed[[\"Meal Type\", \"Dessert\"]],\n",
        "        menu_encoded,\n",
        "        event_encoded\n",
        "    ], axis=1)\n",
        "\n",
        "    # Align columns\n",
        "    aligned_features = features.reindex(columns=preprocessor.get_feature_names_out(), fill_value=0)\n",
        "\n",
        "    # Predict totals\n",
        "    totals = predict_total_leftovers(new_meals)\n",
        "\n",
        "        # After preprocessing in predict_new_meals():\n",
        "    #print(\"Aligned Features Sample:\\n\", aligned_features.head())\n",
        "    #print(\"Feature Means:\\n\", aligned_features.mean(axis=0))\n",
        "\n",
        "\n",
        "    # Distribute using NNLS\n",
        "    nmf, H = train_matrix_factorization(data_subset, mlb_menu)\n",
        "    individual_preds = distribute_leftovers(totals, menu_encoded.values, H)\n",
        "    # Save NMF model\n",
        "    joblib.dump(nmf, \"/content/drive/MyDrive/육사 부식 잔반 최적화/nmf_model.pkl\")\n",
        "\n",
        "    # Also save the item factors matrix H\n",
        "    joblib.dump(nmf.components_, \"/content/drive/MyDrive/육사 부식 잔반 최적화/nmf_components.pkl\")\n",
        "\n",
        "    # Add return statement\n",
        "    return pd.DataFrame(individual_preds, columns=all_items), totals\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CkaDiT2I8lAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧪 Example Usage\n",
        "test_meals = pd.DataFrame([\n",
        "    {\n",
        "        \"Meal Type\": \"B\",\n",
        "        \"Menu\": \"베이컨불고기치즈버거,닭다리후라이드,치킨무나쵸소스,푸실리샐러드,시리얼\",\n",
        "        \"Dessert\": \"1\",\n",
        "        \"Event\": \"3\"\n",
        "    }\n",
        "])\n",
        "individual, total = predict_new_meals(test_meals)\n",
        "print(f\"Total Prediction: {total[0]:.2f}\")\n",
        "print(\"Individual Contributions:\")\n",
        "print(individual.loc[:, individual.iloc[0] > 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg6WlcFW8l-r",
        "outputId": "98cbcf64-30fc-4c71-9f89-98d16db259fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Prediction: 83.12\n",
            "Individual Contributions:\n",
            "   베이컨불고기치즈버거  닭다리후라이드  푸실리샐러드  시리얼  치킨무나쵸소스\n",
            "0          20       14      14   19       14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error Bound"
      ],
      "metadata": {
        "id": "katmK7RH8nxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmf = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/nmf_model.pkl\")"
      ],
      "metadata": {
        "id": "awFq-VBe8nHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import nnls\n",
        "import joblib\n",
        "\n",
        "# Load all components\n",
        "nmf = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/nmf_model.pkl\")\n",
        "voting_regressor = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/voting_regressor.pkl\")\n",
        "preprocessor = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/preprocessor.pkl\")\n",
        "mlb_menu = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_event.pkl\")\n",
        "\n",
        "# Load dataset with true leftovers\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/육사 부식 잔반 최적화/current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([item.strip() for item in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([item.strip() for item in x.split(\",\")]))\n",
        "\n",
        "\n",
        "def predict_total_leftovers(new_meals):\n",
        "    processed = new_meals.copy()\n",
        "    processed[\"Menu\"] = processed[\"Menu\"].str.split(\",\").apply(lambda x: [i.strip() for i in x])\n",
        "    processed[\"Event\"] = processed[\"Event\"].str.split(\",\").apply(lambda x: [i.strip() for i in x])\n",
        "\n",
        "    # Compute Menu Avg Leftovers from historical dictionary\n",
        "    menu_avg_leftovers = data.explode(\"Menu\").groupby(\"Menu\")[\"leftovers\"].mean().to_dict()\n",
        "    processed[\"Menu Avg Leftovers\"] = processed[\"Menu\"].apply(\n",
        "        lambda items: np.mean([menu_avg_leftovers.get(item, 0) for item in items])\n",
        "    )\n",
        "\n",
        "    # Encoding\n",
        "    menu_encoded = pd.DataFrame(\n",
        "        mlb_menu.transform(processed[\"Menu\"]),\n",
        "        columns=mlb_menu.classes_,\n",
        "        index=processed.index\n",
        "    )\n",
        "    event_encoded = pd.DataFrame(\n",
        "        mlb_event.transform(processed[\"Event\"]),\n",
        "        columns=mlb_event.classes_,\n",
        "        index=processed.index\n",
        "    )\n",
        "\n",
        "    # Concatenate features\n",
        "    features = pd.concat([\n",
        "        processed[[\"Meal Type\", \"Dessert\", \"Menu Avg Leftovers\"]],\n",
        "        menu_encoded,\n",
        "        event_encoded\n",
        "    ], axis=1)\n",
        "\n",
        "    X_input = preprocessor.transform(features)\n",
        "    total_preds = np.expm1(voting_regressor.predict(X_input))\n",
        "\n",
        "    return total_preds[0], menu_encoded, processed\n",
        "\n",
        "\n",
        "def predict_with_bounds_and_theory(new_meals):\n",
        "    total_pred, menu_encoded, processed = predict_total_leftovers(new_meals)\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "    if not active_idx.size:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    H_active = H[:, active_idx]  # shape: (r, |active|)\n",
        "    cond_H = np.linalg.cond(H_active)  # from paper: κ(H)\n",
        "\n",
        "    # Compute NNLS weights\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    # Get total_true by matching record\n",
        "    matched = data[\n",
        "        (data[\"Meal Type\"] == processed[\"Meal Type\"].iloc[0]) &\n",
        "        (data[\"Dessert\"] == processed[\"Dessert\"].iloc[0]) &\n",
        "        (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(processed[\"Menu\"].iloc[0]))) &\n",
        "        (data[\"Event\"].apply(lambda x: sorted(x) == sorted(processed[\"Event\"].iloc[0])))\n",
        "    ]\n",
        "    if not matched.empty:\n",
        "        total_true = matched[\"leftovers\"].values[0]\n",
        "    else:\n",
        "        total_true = total_pred  # fallback: assume perfect match\n",
        "\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    # Estimate e_i from full reconstruction error\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    #recon_error_vector = np.abs(V - V_hat).mean(axis=0)  # per-item NMF error\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)  # per-item NMF error\n",
        "\n",
        "    # Final bound from theory: |x_i - x*_i| <= κ(H) * |T_hat - T*| + |e_i|\n",
        "    results = []\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "        results.append({\n",
        "            \"item\": mlb_menu.classes_[idx],\n",
        "            \"prediction\": pred,\n",
        "            \"margin\": bound,\n",
        "            \"percent_error\": (bound / (pred + 1e-8)) * 100\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KU1NtfWY8pzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Example usage\n",
        "test_meals = pd.DataFrame([{\n",
        "    \"Meal Type\": \"D\",\n",
        "    \"Menu\": \"영양밥,콩나물국,비엔나소시지야채볶음,돼지고기감자조림\",\n",
        "    \"Dessert\": \"0\",\n",
        "    \"Event\": \"3\"\n",
        "}])\n",
        "\n",
        "result_df = predict_with_bounds_and_theory(test_meals)\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in5k7LsB8qmY",
        "outputId": "761e49de-3eee-4ce6-dcd3-818b70276165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         item  prediction    margin  percent_error\n",
            "0    돼지고기감자조림   56.617107  0.545452       0.963404\n",
            "1  비엔나소시지야채볶음   20.511335  0.500000       2.437677\n",
            "2         영양밥   10.086863  0.000068       0.000676\n",
            "3        콩나물국   29.822491  0.500930       1.679705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program"
      ],
      "metadata": {
        "id": "V9k7RpGJ9B0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 맨 위 library install 하기\n",
        "2. 세션 재시작하기"
      ],
      "metadata": {
        "id": "6aZmXPvdbFyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict_leftovers_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Load models and encoders\n",
        "best_pipeline = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/final_pipeline.pkl\")\n",
        "menu_avg_leftovers = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/menu_avg_leftovers.pkl\")\n",
        "mlb_menu = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_event.pkl\")\n",
        "nmf = joblib.load(\"/content/drive/MyDrive/육사 부식 잔반 최적화/nmf_model.pkl\")\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/육사 부식 잔반 최적화/current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "\n",
        "DESSERT_MAP = {\"없음\": \"0\", \"유제품\": \"1\", \"과일\": \"2\", \"과일푸딩\": \"3\", \"이온음료/ 에이드/ 탄산\": \"4\", \"핫바\": \"5\", \"마카롱/ 초콜릿/ 에너지바\": \"6\"}\n",
        "EVENT_MAP = {\"주말, 공휴일\": \"1\", \"주중\": \"0\", \"유격\": \"4\", \"중대 전술훈련 및 기본 훈련\": \"3\"}\n",
        "MEAL_TYPE_MAP = {\"아침\": \"A\", \"점심\": \"B\", \"저녁\": \"C\", \"브런치\": \"D\"}\n",
        "\n",
        "def predict_leftovers(meal_type, menu_items, dessert, event):\n",
        "    menu_items_list = [i.strip() for i in menu_items.split(\",\")]\n",
        "    known_menu_items = [item for item in menu_items_list if item in menu_avg_leftovers]\n",
        "    if not known_menu_items:\n",
        "        st.error(\"Error: Unknown menu items.\")\n",
        "        return {}\n",
        "\n",
        "    avg_leftovers = np.mean([menu_avg_leftovers[item] for item in known_menu_items])\n",
        "    raw_input = pd.DataFrame({\n",
        "        'Meal Type': [meal_type],\n",
        "        'Dessert': [dessert],\n",
        "        'Menu Avg Leftovers': [avg_leftovers]\n",
        "    })\n",
        "\n",
        "    menu_encoded = pd.DataFrame(mlb_menu.transform([menu_items_list]), columns=mlb_menu.classes_)\n",
        "    event_encoded = pd.DataFrame(mlb_event.transform([[event]]), columns=mlb_event.classes_)\n",
        "    input_df = pd.concat([raw_input, menu_encoded, event_encoded], axis=1)\n",
        "    input_np = best_pipeline.named_steps['preprocessor'].transform(input_df)\n",
        "\n",
        "    total_pred = best_pipeline.named_steps['regressor'].predict(input_np)[0]\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "    H_active = H[:, active_idx]\n",
        "    cond_H = np.linalg.cond(H_active)\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    matched = data[(data[\"Meal Type\"] == meal_type) &\n",
        "                   (data[\"Dessert\"] == dessert) &\n",
        "                   (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(menu_items_list))) &\n",
        "                   (data[\"Event\"].apply(lambda x: sorted(x) == sorted([event])))]\n",
        "\n",
        "    total_true = matched[\"leftovers\"].values[0] if not matched.empty else total_pred\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)\n",
        "\n",
        "    predictions = {}\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        item = mlb_menu.classes_[idx]\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "\n",
        "        # ✅ bound가 NaN 또는 inf이면 0으로 고정\n",
        "        if not np.isfinite(bound):\n",
        "            bound = 0.0\n",
        "\n",
        "        percent = (bound / (pred + 1e-8)) * 100\n",
        "        if not np.isfinite(percent) or percent > 100:\n",
        "            percent = 0.0\n",
        "\n",
        "        predictions[item] = f\"{pred:.1f} ± {bound:.1f} ({percent:.1f}%)\"\n",
        "\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# 🎨 Streamlit UI with Korean Labels\n",
        "st.title(\"🍛 군 급식 잔반 예측 프로그램\")\n",
        "st.markdown(\"메뉴 정보를 입력하면 예상 잔반량을 예측합니다.\")\n",
        "\n",
        "meal_type_korean = st.selectbox(\"🍽️ 식사 종류 선택\", list(MEAL_TYPE_MAP.keys()))\n",
        "meal_type = MEAL_TYPE_MAP[meal_type_korean]\n",
        "\n",
        "menu_items = st.text_input(\"🍲 메뉴 항목 입력 (쉼표로 구분)\", \"영양밥,콩나물국,비엔나소시지야채볶음,돼지고기감자조림\")\n",
        "\n",
        "dessert_korean = st.selectbox(\"🍰 디저트 선택\", list(DESSERT_MAP.keys()))\n",
        "dessert = DESSERT_MAP[dessert_korean]\n",
        "\n",
        "event_korean = st.selectbox(\"🎯 행사 선택\", list(EVENT_MAP.keys()))\n",
        "event = EVENT_MAP[event_korean]\n",
        "\n",
        "# ✅ NEW: Enter number of people\n",
        "num_people = st.number_input(\"👥 식사 인원 수\", min_value=1, value=100)\n",
        "\n",
        "\n",
        "# ✅ Predict button\n",
        "if st.button(\"🧮 예측하기\"):\n",
        "    with st.spinner(\"계산 중...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people / 1000:.2f} kg ± {float(v.split()[2]) * num_people / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(\"✅ 예측 완료!\")\n",
        "            st.write(\"### 🍽️ 예상 잔반량 (각 메뉴별)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "# 🔧 Additional Percentage Slider and Button\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"🔧 특정 비율로 잔반량 계산\")\n",
        "\n",
        "percentage = st.slider(\"🔧 예측 잔반의 몇 퍼센트를 반환할까요?\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"🔄 특정 비율로 잔반 계산하기\"):\n",
        "    with st.spinner(\"계산 중...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people * (percentage / 100) / 1000:.2f} kg ± {float(v.split()[2]) * num_people * (percentage / 100) / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(f\"✅ 예측 완료! ({percentage}% 기준)\")\n",
        "            st.write(f\"### 🍽️ 예상 잔반량 - {percentage}% 기준 (각 메뉴별)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n32dLCec9teA",
        "outputId": "5e5c93e2-7a33-4977-c3a5-113f61c59d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting predict_leftovers_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run predict_leftovers_app.py --server.port 8501 &\n",
        "!ngrok config add-authtoken 2uO7qKTmyrri0YTq05KzyH0BWBW_7RhnrTVfK3JfGVzfHyCCq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnLHKty69EgP",
        "outputId": "6aa0ed17-2d81-4f4d-9cc1-5b1aa0afd2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill any previous ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a new ngrok tunnel (fixing the API format issue)\n",
        "public_url = ngrok.connect(8501, \"http\")  # ✅ FIX: Use `http` instead of `port`\n",
        "\n",
        "print(\"🚀 Streamlit App is running at:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RpgZvdZ9FZP",
        "outputId": "652a7a46-dc26-44c8-d30a-ee6fa96437ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Streamlit App is running at: NgrokTunnel: \"https://0db1-35-236-163-7.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyinstaller"
      ],
      "metadata": {
        "id": "reEqB3ZN-R9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326d22f1-52ab-49f2-9639-018a4b1038e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyinstaller\n",
            "  Downloading pyinstaller-6.13.0-py3-none-manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: setuptools>=42.0.0 in /usr/local/lib/python3.11/dist-packages (from pyinstaller) (75.2.0)\n",
            "Collecting altgraph (from pyinstaller)\n",
            "  Downloading altgraph-0.17.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pyinstaller-hooks-contrib>=2025.2 (from pyinstaller)\n",
            "  Downloading pyinstaller_hooks_contrib-2025.3-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from pyinstaller) (24.2)\n",
            "Downloading pyinstaller-6.13.0-py3-none-manylinux2014_x86_64.whl (721 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.0/721.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyinstaller_hooks_contrib-2025.3-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.3/434.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading altgraph-0.17.4-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: altgraph, pyinstaller-hooks-contrib, pyinstaller\n",
            "Successfully installed altgraph-0.17.4 pyinstaller-6.13.0 pyinstaller-hooks-contrib-2025.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# 압축할 폴더 생성\n",
        "os.makedirs(\"/content/offline_app\", exist_ok=True)\n",
        "\n",
        "# ✅ Streamlit 앱 코드 저장\n",
        "with open(\"/content/offline_app/predict_leftovers_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\n",
        "\n",
        "\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Load models and encoders\n",
        "best_pipeline = joblib.load(\"final_pipeline.pkl\")\n",
        "menu_avg_leftovers = joblib.load(\"menu_avg_leftovers.pkl\")\n",
        "mlb_menu = joblib.load(\"mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"mlb_event.pkl\")\n",
        "nmf = joblib.load(\"nmf_model.pkl\")\n",
        "data = pd.read_excel(\"current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "\n",
        "DESSERT_MAP = {\"없음\": \"0\", \"유제품\": \"1\", \"과일\": \"2\", \"과일푸딩\": \"3\", \"이온음료/ 에이드/ 탄산\": \"4\", \"핫바\": \"5\", \"마카롱/ 초콜릿/ 에너지바\": \"6\"}\n",
        "EVENT_MAP = {\"주말, 공휴일\": \"1\", \"주중\": \"0\", \"유격\": \"4\", \"중대 전술훈련 및 기본 훈련\": \"3\"}\n",
        "MEAL_TYPE_MAP = {\"아침\": \"A\", \"점심\": \"B\", \"저녁\": \"C\", \"브런치\": \"D\"}\n",
        "\n",
        "def predict_leftovers(meal_type, menu_items, dessert, event):\n",
        "    menu_items_list = [i.strip() for i in menu_items.split(\",\")]\n",
        "    known_menu_items = [item for item in menu_items_list if item in menu_avg_leftovers]\n",
        "    if not known_menu_items:\n",
        "        st.error(\"Error: Unknown menu items.\")\n",
        "        return {}\n",
        "\n",
        "    avg_leftovers = np.mean([menu_avg_leftovers[item] for item in known_menu_items])\n",
        "    raw_input = pd.DataFrame({\n",
        "        'Meal Type': [meal_type],\n",
        "        'Dessert': [dessert],\n",
        "        'Menu Avg Leftovers': [avg_leftovers]\n",
        "    })\n",
        "\n",
        "    menu_encoded = pd.DataFrame(mlb_menu.transform([menu_items_list]), columns=mlb_menu.classes_)\n",
        "    event_encoded = pd.DataFrame(mlb_event.transform([[event]]), columns=mlb_event.classes_)\n",
        "    input_df = pd.concat([raw_input, menu_encoded, event_encoded], axis=1)\n",
        "    input_np = best_pipeline.named_steps['preprocessor'].transform(input_df)\n",
        "\n",
        "    total_pred = best_pipeline.named_steps['regressor'].predict(input_np)[0]\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "    H_active = H[:, active_idx]\n",
        "    cond_H = np.linalg.cond(H_active)\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    matched = data[(data[\"Meal Type\"] == meal_type) &\n",
        "                   (data[\"Dessert\"] == dessert) &\n",
        "                   (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(menu_items_list))) &\n",
        "                   (data[\"Event\"].apply(lambda x: sorted(x) == sorted([event])))]\n",
        "\n",
        "    total_true = matched[\"leftovers\"].values[0] if not matched.empty else total_pred\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)\n",
        "\n",
        "    predictions = {}\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        item = mlb_menu.classes_[idx]\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "\n",
        "        # ✅ bound가 NaN 또는 inf이면 0으로 고정\n",
        "        if not np.isfinite(bound):\n",
        "            bound = 0.0\n",
        "\n",
        "        percent = (bound / (pred + 1e-8)) * 100\n",
        "        if not np.isfinite(percent) or percent > 100:\n",
        "            percent = 0.0\n",
        "\n",
        "        predictions[item] = f\"{pred:.1f} ± {bound:.1f} ({percent:.1f}%)\"\n",
        "\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# 🎨 Streamlit UI with Korean Labels\n",
        "st.title(\"🍛 군 급식 잔반 예측 프로그램\")\n",
        "st.markdown(\"메뉴 정보를 입력하면 예상 잔반량을 예측합니다.\")\n",
        "\n",
        "meal_type_korean = st.selectbox(\"🍽️ 식사 종류 선택\", list(MEAL_TYPE_MAP.keys()))\n",
        "meal_type = MEAL_TYPE_MAP[meal_type_korean]\n",
        "\n",
        "menu_items = st.text_input(\"🍲 메뉴 항목 입력 (쉼표로 구분)\", \"영양밥,콩나물국,비엔나소시지야채볶음,돼지고기감자조림\")\n",
        "\n",
        "dessert_korean = st.selectbox(\"🍰 디저트 선택\", list(DESSERT_MAP.keys()))\n",
        "dessert = DESSERT_MAP[dessert_korean]\n",
        "\n",
        "event_korean = st.selectbox(\"🎯 행사 선택\", list(EVENT_MAP.keys()))\n",
        "event = EVENT_MAP[event_korean]\n",
        "\n",
        "# ✅ NEW: Enter number of people\n",
        "num_people = st.number_input(\"👥 식사 인원 수\", min_value=1, value=100)\n",
        "\n",
        "\n",
        "# ✅ Predict button\n",
        "if st.button(\"🧮 예측하기\"):\n",
        "    with st.spinner(\"계산 중...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people / 1000:.2f} kg ± {float(v.split()[2]) * num_people / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(\"✅ 예측 완료!\")\n",
        "            st.write(\"### 🍽️ 예상 잔반량 (각 메뉴별)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "# 🔧 Additional Percentage Slider and Button\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"🔧 특정 비율로 잔반량 계산\")\n",
        "\n",
        "percentage = st.slider(\"🔧 예측 잔반의 몇 퍼센트를 반환할까요?\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"🔄 특정 비율로 잔반 계산하기\"):\n",
        "    with st.spinner(\"계산 중...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people * (percentage / 100) / 1000:.2f} kg ± {float(v.split()[2]) * num_people * (percentage / 100) / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(f\"✅ 예측 완료! ({percentage}% 기준)\")\n",
        "            st.write(f\"### 🍽️ 예상 잔반량 - {percentage}% 기준 (각 메뉴별)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# ✅ requirements.txt 생성\n",
        "with open(\"/content/offline_app/requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "streamlit==1.32.2\n",
        "scikit-learn==1.4.2\n",
        "numpy==1.26.4\n",
        "pandas==2.2.2\n",
        "joblib==1.4.0\n",
        "openpyxl==3.1.2\n",
        "scipy==1.13.0\n",
        "xgboost==2.0.3\n",
        "catboost==1.2.7\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# ✅ 모델과 데이터 복사\n",
        "file_paths = [\n",
        "    \"/content/drive/MyDrive/육사 부식 잔반 최적화/final_pipeline.pkl\",\n",
        "    \"/content/drive/MyDrive/육사 부식 잔반 최적화/menu_avg_leftovers.pkl\",\n",
        "    \"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_menu.pkl\",\n",
        "    \"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_event.pkl\",\n",
        "    \"/content/drive/MyDrive/육사 부식 잔반 최적화/nmf_model.pkl\",\n",
        "    \"/content/drive/MyDrive/육사 부식 잔반 최적화/current_processed_menu_data.xlsx\",\n",
        "]\n",
        "\n",
        "for path in file_paths:\n",
        "    shutil.copy(path, \"/content/offline_app\")\n",
        "\n",
        "# ✅ 압축하기\n",
        "with ZipFile(\"/content/predict_leftovers_offline_package.zip\", \"w\") as zipf:\n",
        "    for fname in os.listdir(\"/content/offline_app\"):\n",
        "        zipf.write(os.path.join(\"/content/offline_app\", fname), arcname=fname)\n"
      ],
      "metadata": {
        "id": "ZagIwio9ogIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/offline_app.zip /content/offline_app\n",
        "from google.colab import files\n",
        "files.download(\"/content/offline_app.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "p1hJVefKrli_",
        "outputId": "367f93f2-03db-4b5c-f0db-eae37cecf1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/offline_app/ (stored 0%)\n",
            "updating: content/offline_app/mlb_menu.pkl (deflated 63%)\n",
            "updating: content/offline_app/requirements.txt (deflated 24%)\n",
            "updating: content/offline_app/predict_leftovers_app.py (deflated 60%)\n",
            "updating: content/offline_app/final_pipeline.pkl (deflated 76%)\n",
            "updating: content/offline_app/current_processed_menu_data.xlsx (deflated 8%)\n",
            "updating: content/offline_app/mlb_event.pkl (deflated 29%)\n",
            "updating: content/offline_app/nmf_model.pkl (deflated 99%)\n",
            "updating: content/offline_app/menu_avg_leftovers.pkl (deflated 66%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d41fc98-7406-4113-a98b-ad8a5b83dbef\", \"offline_app.zip\", 849433)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Load models and encoders\n",
        "best_pipeline = joblib.load(\"final_pipeline.pkl\")\n",
        "menu_avg_leftovers = joblib.load(\"menu_avg_leftovers.pkl\")\n",
        "mlb_menu = joblib.load(\"mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"mlb_event.pkl\")\n",
        "nmf = joblib.load(\"nmf_model.pkl\")\n",
        "data = pd.read_excel(\"current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "\n",
        "DESSERT_MAP = {\"없음\": \"0\", \"유제품\": \"1\", \"과일\": \"2\", \"과일푸딩\": \"3\", \"이온음료/ 에이드/ 탄산\": \"4\", \"핫바\": \"5\", \"마카롱/ 초콜릿/ 에너지바\": \"6\"}\n",
        "EVENT_MAP = {\"주말, 공휴일\": \"1\", \"주중\": \"0\", \"유격\": \"4\", \"중대 전술훈련 및 기본 훈련\": \"3\"}\n",
        "MEAL_TYPE_MAP = {\"아침\": \"A\", \"점심\": \"B\", \"저녁\": \"C\", \"브런치\": \"D\"}\n",
        "\n",
        "def predict_leftovers(meal_type, menu_items, dessert, event):\n",
        "    menu_items_list = [i.strip() for i in menu_items.split(\",\")]\n",
        "    known_menu_items = [item for item in menu_items_list if item in menu_avg_leftovers]\n",
        "    if not known_menu_items:\n",
        "        st.error(\"Error: Unknown menu items.\")\n",
        "        return {}\n",
        "\n",
        "    avg_leftovers = np.mean([menu_avg_leftovers[item] for item in known_menu_items])\n",
        "    raw_input = pd.DataFrame({\n",
        "        'Meal Type': [meal_type],\n",
        "        'Dessert': [dessert],\n",
        "        'Menu Avg Leftovers': [avg_leftovers]\n",
        "    })\n",
        "\n",
        "    menu_encoded = pd.DataFrame(mlb_menu.transform([menu_items_list]), columns=mlb_menu.classes_)\n",
        "    event_encoded = pd.DataFrame(mlb_event.transform([[event]]), columns=mlb_event.classes_)\n",
        "    input_df = pd.concat([raw_input, menu_encoded, event_encoded], axis=1)\n",
        "    input_np = best_pipeline.named_steps['preprocessor'].transform(input_df)\n",
        "\n",
        "    total_pred = best_pipeline.named_steps['regressor'].predict(input_np)[0]\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "    H_active = H[:, active_idx]\n",
        "    cond_H = np.linalg.cond(H_active)\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    matched = data[(data[\"Meal Type\"] == meal_type) &\n",
        "                   (data[\"Dessert\"] == dessert) &\n",
        "                   (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(menu_items_list))) &\n",
        "                   (data[\"Event\"].apply(lambda x: sorted(x) == sorted([event])))]\n",
        "\n",
        "    total_true = matched[\"leftovers\"].values[0] if not matched.empty else total_pred\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)\n",
        "\n",
        "    predictions = {}\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        item = mlb_menu.classes_[idx]\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "\n",
        "        # ✅ bound가 NaN 또는 inf이면 0으로 고정\n",
        "        if not np.isfinite(bound):\n",
        "            bound = 0.0\n",
        "\n",
        "        percent = (bound / (pred + 1e-8)) * 100\n",
        "        if not np.isfinite(percent) or percent > 100:\n",
        "            percent = 0.0\n",
        "\n",
        "        predictions[item] = f\"{pred:.1f} ± {bound:.1f} ({percent:.1f}%)\"\n",
        "\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# 🎨 Streamlit UI with Korean Labels\n",
        "st.title(\"🍛 군 급식 잔반 예측 프로그램\")\n",
        "st.markdown(\"메뉴 정보를 입력하면 예상 잔반량을 예측합니다.\")\n",
        "\n",
        "meal_type_korean = st.selectbox(\"🍽️ 식사 종류 선택\", list(MEAL_TYPE_MAP.keys()))\n",
        "meal_type = MEAL_TYPE_MAP[meal_type_korean]\n",
        "\n",
        "menu_items = st.text_input(\"🍲 메뉴 항목 입력 (쉼표로 구분)\", \"영양밥,콩나물국,비엔나소시지야채볶음,돼지고기감자조림\")\n",
        "\n",
        "dessert_korean = st.selectbox(\"🍰 디저트 선택\", list(DESSERT_MAP.keys()))\n",
        "dessert = DESSERT_MAP[dessert_korean]\n",
        "\n",
        "event_korean = st.selectbox(\"🎯 행사 선택\", list(EVENT_MAP.keys()))\n",
        "event = EVENT_MAP[event_korean]\n",
        "\n",
        "# ✅ NEW: Enter number of people\n",
        "num_people = st.number_input(\"👥 식사 인원 수\", min_value=1, value=100)\n",
        "\n",
        "\n",
        "# ✅ Predict button\n",
        "if st.button(\"🧮 예측하기\"):\n",
        "    with st.spinner(\"계산 중...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people / 1000:.2f} kg ± {float(v.split()[2]) * num_people / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(\"✅ 예측 완료!\")\n",
        "            st.write(\"### 🍽️ 예상 잔반량 (각 메뉴별)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "# 🔧 Additional Percentage Slider and Button\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"🔧 특정 비율로 잔반량 계산\")\n",
        "\n",
        "percentage = st.slider(\"🔧 예측 잔반의 몇 퍼센트를 반환할까요?\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"🔄 특정 비율로 잔반 계산하기\"):\n",
        "    with st.spinner(\"계산 중...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people * (percentage / 100) / 1000:.2f} kg ± {float(v.split()[2]) * num_people * (percentage / 100) / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(f\"✅ 예측 완료! ({percentage}% 기준)\")\n",
        "            st.write(f\"### 🍽️ 예상 잔반량 - {percentage}% 기준 (각 메뉴별)\")\n",
        "            st.json(scaled_predictions)"
      ],
      "metadata": {
        "id": "nBUTYrv5dSfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# 🔧 오프라인 앱 폴더 초기화\n",
        "base_dir = \"/content/offline_app\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "os.makedirs(f\"{base_dir}/whl\", exist_ok=True)\n",
        "\n",
        "# ✅ 필요한 .whl 파일들 다운로드\n",
        "wheels = {\n",
        "    \"numpy\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/numpy-1.26.4-cp311-cp311-win_amd64.whl\",\n",
        "    \"pandas\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/pandas-2.2.2-cp311-cp311-win_amd64.whl\",\n",
        "    \"scipy\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/scipy-1.13.0-cp311-cp311-win_amd64.whl\",\n",
        "    \"scikit-learn\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/scikit_learn-1.4.2-cp311-cp311-win_amd64.whl\",\n",
        "    \"catboost\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/catboost-1.2.7-cp311-cp311-win_amd64.whl\",\n",
        "    \"xgboost\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/xgboost-2.0.3-cp311-cp311-win_amd64.whl\",\n",
        "    \"openpyxl\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/openpyxl-3.1.2-py3-none-any.whl\",\n",
        "    \"joblib\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/joblib-1.4.0-py3-none-any.whl\",\n",
        "    \"streamlit\": \"https://files.pythonhosted.org/packages/7d/b6/f9e62e508caaa8b5c2c4f6aa4fc6469b1b30761c7fdc6027c07e55d7918d/streamlit-1.32.2-py2.py3-none-any.whl\"\n",
        "}\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "for name, url in wheels.items():\n",
        "    out_path = os.path.join(base_dir, \"whl\", os.path.basename(url))\n",
        "    if not os.path.exists(out_path):\n",
        "        urlretrieve(url, out_path)\n",
        "\n",
        "# ✅ requirements.txt 생성\n",
        "with open(f\"{base_dir}/requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"numpy\n",
        "pandas\n",
        "scipy\n",
        "scikit-learn\n",
        "catboost\n",
        "xgboost\n",
        "openpyxl\n",
        "joblib\n",
        "streamlit\"\"\")\n",
        "\n",
        "# ✅ start_app.bat 생성\n",
        "with open(f\"{base_dir}/start_app.bat\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(r\"\"\"@echo off\n",
        "cd /d \"%~dp0\"\n",
        "python -m venv venv\n",
        "call venv\\Scripts\\activate\n",
        "pip install --upgrade pip\n",
        "pip install --no-index --find-links=whl -r requirements.txt\n",
        "streamlit run predict_leftovers_app.py\n",
        "pause\n",
        "\"\"\")\n",
        "\n",
        "with open(\"/content/offline_app/predict_leftovers_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\n",
        "\n",
        "\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Load models and encoders\n",
        "best_pipeline = joblib.load(\"final_pipeline.pkl\")\n",
        "menu_avg_leftovers = joblib.load(\"menu_avg_leftovers.pkl\")\n",
        "mlb_menu = joblib.load(\"mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"mlb_event.pkl\")\n",
        "nmf = joblib.load(\"nmf_model.pkl\")\n",
        "data = pd.read_excel(\"current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "\n",
        "DESSERT_MAP = {\"없음\": \"0\", \"유제품\": \"1\", \"과일\": \"2\", \"과일푸딩\": \"3\", \"이온음료/ 에이드/ 탄산\": \"4\", \"핫바\": \"5\", \"마카롱/ 초콜릿/ 에너지바\": \"6\"}\n",
        "EVENT_MAP = {\"주말, 공휴일\": \"1\", \"주중\": \"0\", \"유격\": \"4\", \"중대 전술훈련 및 기본 훈련\": \"3\"}\n",
        "MEAL_TYPE_MAP = {\"아침\": \"A\", \"점심\": \"B\", \"저녁\": \"C\", \"브런치\": \"D\"}\n",
        "\n",
        "def predict_leftovers(meal_type, menu_items, dessert, event):\n",
        "    menu_items_list = [i.strip() for i in menu_items.split(\",\")]\n",
        "    known_menu_items = [item for item in menu_items_list if item in menu_avg_leftovers]\n",
        "    if not known_menu_items:\n",
        "        st.error(\"Error: Unknown menu items.\")\n",
        "        return {}\n",
        "\n",
        "    avg_leftovers = np.mean([menu_avg_leftovers[item] for item in known_menu_items])\n",
        "    raw_input = pd.DataFrame({\n",
        "        'Meal Type': [meal_type],\n",
        "        'Dessert': [dessert],\n",
        "        'Menu Avg Leftovers': [avg_leftovers]\n",
        "    })\n",
        "\n",
        "    menu_encoded = pd.DataFrame(mlb_menu.transform([menu_items_list]), columns=mlb_menu.classes_)\n",
        "    event_encoded = pd.DataFrame(mlb_event.transform([[event]]), columns=mlb_event.classes_)\n",
        "    input_df = pd.concat([raw_input, menu_encoded, event_encoded], axis=1)\n",
        "    input_np = best_pipeline.named_steps['preprocessor'].transform(input_df)\n",
        "\n",
        "    total_pred = best_pipeline.named_steps['regressor'].predict(input_np)[0]\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "    H_active = H[:, active_idx]\n",
        "    cond_H = np.linalg.cond(H_active)\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    matched = data[(data[\"Meal Type\"] == meal_type) &\n",
        "                   (data[\"Dessert\"] == dessert) &\n",
        "                   (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(menu_items_list))) &\n",
        "                   (data[\"Event\"].apply(lambda x: sorted(x) == sorted([event])))]\n",
        "\n",
        "    total_true = matched[\"leftovers\"].values[0] if not matched.empty else total_pred\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)\n",
        "\n",
        "    predictions = {}\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        item = mlb_menu.classes_[idx]\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "\n",
        "        # ✅ bound가 NaN 또는 inf이면 0으로 고정\n",
        "        if not np.isfinite(bound):\n",
        "            bound = 0.0\n",
        "\n",
        "        percent = (bound / (pred + 1e-8)) * 100\n",
        "        if not np.isfinite(percent) or percent > 100:\n",
        "            percent = 0.0\n",
        "\n",
        "        predictions[item] = f\"{pred:.1f} ± {bound:.1f} ({percent:.1f}%)\"\n",
        "\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# 🎨 Streamlit UI with Korean Labels\n",
        "st.title(\"🍛 군 급식 잔반 예측 프로그램\")\n",
        "st.markdown(\"메뉴 정보를 입력하면 예상 잔반량을 예측합니다.\")\n",
        "\n",
        "meal_type_korean = st.selectbox(\"🍽️ 식사 종류 선택\", list(MEAL_TYPE_MAP.keys()))\n",
        "meal_type = MEAL_TYPE_MAP[meal_type_korean]\n",
        "\n",
        "menu_items = st.text_input(\"🍲 메뉴 항목 입력 (쉼표로 구분)\", \"영양밥,콩나물국,비엔나소시지야채볶음,돼지고기감자조림\")\n",
        "\n",
        "dessert_korean = st.selectbox(\"🍰 디저트 선택\", list(DESSERT_MAP.keys()))\n",
        "dessert = DESSERT_MAP[dessert_korean]\n",
        "\n",
        "event_korean = st.selectbox(\"🎯 행사 선택\", list(EVENT_MAP.keys()))\n",
        "event = EVENT_MAP[event_korean]\n",
        "\n",
        "# ✅ NEW: Enter number of people\n",
        "num_people = st.number_input(\"👥 식사 인원 수\", min_value=1, value=100)\n",
        "\n",
        "\n",
        "# ✅ Predict button\n",
        "if st.button(\"🧮 예측하기\"):\n",
        "    with st.spinner(\"계산 중...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people / 1000:.2f} kg ± {float(v.split()[2]) * num_people / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(\"✅ 예측 완료!\")\n",
        "            st.write(\"### 🍽️ 예상 잔반량 (각 메뉴별)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "# 🔧 Additional Percentage Slider and Button\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"🔧 특정 비율로 잔반량 계산\")\n",
        "\n",
        "percentage = st.slider(\"🔧 예측 잔반의 몇 퍼센트를 반환할까요?\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"🔄 특정 비율로 잔반 계산하기\"):\n",
        "    with st.spinner(\"계산 중...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people * (percentage / 100) / 1000:.2f} kg ± {float(v.split()[2]) * num_people * (percentage / 100) / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(f\"✅ 예측 완료! ({percentage}% 기준)\")\n",
        "            st.write(f\"### 🍽️ 예상 잔반량 - {percentage}% 기준 (각 메뉴별)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# ✅ 모델 및 데이터 파일 복사\n",
        "file_paths = {\n",
        "    \"final_pipeline.pkl\": \"/content/drive/MyDrive/육사 부식 잔반 최적화/final_pipeline.pkl\",\n",
        "    \"menu_avg_leftovers.pkl\": \"/content/drive/MyDrive/육사 부식 잔반 최적화/menu_avg_leftovers.pkl\",\n",
        "    \"mlb_menu.pkl\": \"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_menu.pkl\",\n",
        "    \"mlb_event.pkl\": \"/content/drive/MyDrive/육사 부식 잔반 최적화/mlb_event.pkl\",\n",
        "    \"nmf_model.pkl\": \"/content/drive/MyDrive/육사 부식 잔반 최적화/nmf_model.pkl\",\n",
        "    \"current_processed_menu_data.xlsx\": \"/content/drive/MyDrive/육사 부식 잔반 최적화/current_processed_menu_data.xlsx\",\n",
        "}\n",
        "\n",
        "for fname, origin in file_paths.items():\n",
        "    shutil.copy(origin, os.path.join(base_dir, fname))\n",
        "\n",
        "# ✅ 압축하기\n",
        "zip_path = \"/content/offline_app_full.zip\"\n",
        "with ZipFile(zip_path, \"w\") as zipf:\n",
        "    for folder, _, files in os.walk(base_dir):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(folder, file)\n",
        "            rel_path = os.path.relpath(full_path, base_dir)\n",
        "            zipf.write(full_path, arcname=rel_path)\n",
        "\n",
        "# ✅ 다운로드 링크 제공\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "1reAnsC5u_eK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "7dcabf91-67e7-43cd-a5a9-b5b3e436bc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ec2145cee849>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# ✅ requirements.txt 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZgRdqE2pb8g3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}