{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7-cgG5iK8DHq",
        "outputId": "004425a6-b828-4a94-f1bb-21dbb11dc519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting catboost==1.2.7\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (3.10.0)\n",
            "Collecting numpy<2.0,>=1.16.0 (from catboost==1.2.7)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (1.14.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost==1.2.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost==1.2.7) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost==1.2.7) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost==1.2.7) (9.1.2)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, catboost\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed catboost-1.2.7 numpy-1.26.4\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit --quiet\n",
        "!pip install streamlit pyngrok --quiet\n",
        "!pip install catboost==1.2.7\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Voting* Regressor"
      ],
      "metadata": {
        "id": "MxvadfCH8YL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "# âœ… Load the data\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/current_processed_menu_data.xlsx\")\n",
        "\n",
        "# âœ… Select relevant columns and drop NaN values\n",
        "data_subset = data[['Meal Type', 'Menu', 'Dessert', 'Event', 'leftovers']].dropna()\n",
        "\n",
        "# âœ… Convert categorical columns to string\n",
        "data_subset['Meal Type'] = data_subset['Meal Type'].astype(str)\n",
        "data_subset['Dessert'] = data_subset['Dessert'].astype(str)\n",
        "\n",
        "# âœ… Convert multi-label categorical columns to lists\n",
        "data_subset['Menu'] = data_subset['Menu'].astype(str).apply(lambda x: x.split(','))\n",
        "data_subset['Event'] = data_subset['Event'].astype(str).apply(lambda x: x.split(','))\n",
        "\n",
        "# âœ… Compute historical average leftovers per menu item\n",
        "menu_avg_leftovers = data_subset.explode('Menu').groupby('Menu')['leftovers'].mean().to_dict()\n",
        "data_subset['Menu Avg Leftovers'] = data_subset['Menu'].apply(\n",
        "    lambda items: np.mean([menu_avg_leftovers.get(item, 0) for item in items])\n",
        ")\n",
        "\n",
        "# âœ… One-hot encode multi-label categorical variables\n",
        "mlb_menu = MultiLabelBinarizer()\n",
        "mlb_event = MultiLabelBinarizer()\n",
        "menu_encoded = pd.DataFrame(mlb_menu.fit_transform(data_subset['Menu']), columns=mlb_menu.classes_)\n",
        "event_encoded = pd.DataFrame(mlb_event.fit_transform(data_subset['Event']), columns=mlb_event.classes_)\n",
        "\n",
        "# âœ… Concatenate encoded menu & event features with other categorical features\n",
        "data_encoded = pd.concat([data_subset.drop(columns=['Menu', 'Event']), menu_encoded, event_encoded], axis=1)\n",
        "\n",
        "# âœ… Define categorical columns\n",
        "categorical_features = ['Meal Type', 'Dessert']\n",
        "\n",
        "# âœ… Preprocessing pipeline: One-hot encode categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
        "        ('scale', StandardScaler(), ['Menu Avg Leftovers'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# âœ… Apply preprocessing\n",
        "X = preprocessor.fit_transform(data_encoded.drop(columns=['leftovers']))\n",
        "\n",
        "# âœ… Convert processed X into a DataFrame\n",
        "X = pd.DataFrame(X, columns=preprocessor.get_feature_names_out())\n",
        "\n",
        "# âœ… Define target variable (log-transformed leftovers)\n",
        "y_total = np.log1p(data_encoded[['leftovers']])\n",
        "\n",
        "# âœ… Train-test split for total leftovers prediction\n",
        "X_train, X_test, y_train_total, y_test_total = train_test_split(X, y_total, test_size=0.2, random_state=42)\n",
        "\n",
        "# âœ… Train a new Voting Regressor\n",
        "voting_regressor = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('XGBoost', XGBRegressor(n_estimators=300, learning_rate=0.01, max_depth=4, objective=\"reg:squarederror\", random_state=42)),\n",
        "        ('CatBoost', CatBoostRegressor(iterations=500, learning_rate=0.01, depth=6, verbose=0, random_state=42)),\n",
        "        ('RandomForest', RandomForestRegressor(n_estimators=300, max_depth=20, min_samples_leaf=5, random_state=42))\n",
        "    ]\n",
        ")\n",
        "\n",
        "voting_regressor.fit(X_train, y_train_total.values.ravel())\n",
        "y_pred_total = np.expm1(voting_regressor.predict(X_test))\n",
        "\n",
        "# âœ… Compute RÂ² score for Voting Regressor\n",
        "regressor_r2 = r2_score(y_test_total, np.log1p(y_pred_total))\n",
        "print(f\"Voting Regressor RÂ²: {regressor_r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0PZvUaq8VtN",
        "outputId": "f115eac0-8d4d-4a8a-b126-c9a1b1d4f75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Regressor RÂ²: 0.6179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained Voting Regressor\n",
        "joblib.dump(voting_regressor, \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/voting_regressor.pkl\")\n",
        "\n",
        "# Save the preprocessor\n",
        "joblib.dump(preprocessor, \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/preprocessor.pkl\")\n",
        "\n",
        "# Save MultiLabelBinarizers\n",
        "joblib.dump(mlb_menu, \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_menu.pkl\")\n",
        "joblib.dump(mlb_event, \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_event.pkl\")\n",
        "\n",
        "print(\"All components saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZocaeJ68ayo",
        "outputId": "40c001c1-1dee-49c6-c633-56db71ded3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All components saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ğŸ”„ Reload trained components\n",
        "voting_regressor = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/voting_regressor.pkl\")\n",
        "preprocessor = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/preprocessor.pkl\")\n",
        "mlb_menu = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_event.pkl\")\n",
        "model = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/voting_regressor.pkl\")\n"
      ],
      "metadata": {
        "id": "04EtLFox8cQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix Factorization + NNLS"
      ],
      "metadata": {
        "id": "FSpLpqT38jZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê³ ìœ  ìŒì‹ ì¢…ë¥˜ ê°œìˆ˜ ì¶œë ¥\n",
        "unique_items = set(item.strip() for sublist in data_subset['Menu'] for item in sublist)\n",
        "print(f\"ì´ ê³ ìœ  ìŒì‹ ì¢…ë¥˜ ìˆ˜: {len(unique_items)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZIdRf-r8hsQ",
        "outputId": "88e6c379-4976-4be2-cc75-63a94aa37659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ ê³ ìœ  ìŒì‹ ì¢…ë¥˜ ìˆ˜: 694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import nnls\n",
        "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import joblib\n",
        "\n",
        "\n",
        "def train_matrix_factorization(data_subset, mlb_menu, n_factors=700):  #n_factors ì¤„ì´ë©´ ë” ê·¸ëŸ´ì‹¸í•´ë³´ì„\n",
        "    \"\"\"Train NMF with robust normalization to avoid division by zero\"\"\"\n",
        "    historical_menu_encoded = mlb_menu.transform(data_subset['Menu'])\n",
        "\n",
        "    # Ensure valid factorization dimensions\n",
        "    n_samples, n_features = historical_menu_encoded.shape\n",
        "    n_factors = min(n_factors, n_samples, n_features)\n",
        "\n",
        "    nmf = NMF(\n",
        "        n_components=n_factors,\n",
        "        init='nndsvda',\n",
        "        solver='mu',\n",
        "        beta_loss='kullback-leibler',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    W = nmf.fit_transform(historical_menu_encoded)\n",
        "    H = nmf.components_\n",
        "\n",
        "    # Stable normalization with condition number check\n",
        "    norms = np.linalg.norm(H, axis=0, keepdims=True)\n",
        "    H_normalized = H / np.where(norms < 1e-10, 1e-10, norms)\n",
        "\n",
        "    return nmf, H_normalized\n",
        "\n",
        "# ================== NNLS Distribution ==================\n",
        "def distribute_leftovers(total_leftovers, menu_encoded_values, H):\n",
        "    \"\"\"Distribute totals using NNLS with item interactions\"\"\"\n",
        "    individual_preds = []\n",
        "\n",
        "    for total, menu_vec in zip(total_leftovers, menu_encoded_values):\n",
        "        active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "        if not active_idx.size:\n",
        "            individual_preds.append(np.zeros_like(menu_vec))\n",
        "            continue\n",
        "\n",
        "        # Use latent factors for active items\n",
        "        H_sub = H[:, active_idx]\n",
        "        A = H_sub.T @ H_sub + 1e-6 * np.eye(H_sub.shape[1])  # Regularization\n",
        "        b = H_sub.T @ np.ones(H_sub.shape[0]) * total\n",
        "\n",
        "        weights, _ = nnls(A, b)\n",
        "\n",
        "        # Force sum to match total leftovers exactly\n",
        "        weights /= weights.sum() + 1e-10  # Normalize weights to sum to 1\n",
        "        weights *= total                 # Scale weights to match total\n",
        "\n",
        "        full_weights = np.zeros_like(menu_vec)\n",
        "        full_weights[active_idx] = weights\n",
        "\n",
        "        individual_preds.append(full_weights)\n",
        "\n",
        "    return np.array(individual_preds)\n",
        "\n",
        "def predict_new_meals(new_meals):\n",
        "\n",
        "    def predict_total_leftovers(new_meals):\n",
        "      \"\"\"\n",
        "      Predicts total leftovers for new meal combinations using the Voting Regressor.\n",
        "      Handles unseen combinations of known menu items.\n",
        "      \"\"\"\n",
        "      # Preprocess new data to match training structure\n",
        "      processed = new_meals.copy()\n",
        "      processed[\"Menu\"] = processed[\"Menu\"].str.split(\",\")\n",
        "      processed[\"Event\"] = processed[\"Event\"].str.split(\",\")\n",
        "\n",
        "      # Filter to known menu/event items from training data\n",
        "      valid_menu = set(mlb_menu.classes_)\n",
        "      valid_events = set(mlb_event.classes_)\n",
        "      processed[\"Menu\"] = processed[\"Menu\"].apply(\n",
        "          lambda x: [item.strip() for item in x if item.strip() in valid_menu]\n",
        "      )\n",
        "      processed[\"Event\"] = processed[\"Event\"].apply(\n",
        "          lambda x: [item.strip() for item in x if item.strip() in valid_events]\n",
        "      )\n",
        "\n",
        "      # Compute \"Menu Avg Leftovers\" using historical training averages\n",
        "      processed[\"Menu Avg Leftovers\"] = processed[\"Menu\"].apply(\n",
        "          lambda items: np.mean([menu_avg_leftovers.get(item, 0) for item in items])\n",
        "      )\n",
        "\n",
        "      # Encode menu and event with MultiLabelBinarizer (ensure all training columns exist)\n",
        "      menu_encoded = pd.DataFrame(\n",
        "          mlb_menu.transform(processed[\"Menu\"]),\n",
        "          columns=mlb_menu.classes_,\n",
        "          index=processed.index\n",
        "      ).reindex(columns=mlb_menu.classes_, fill_value=0)  # Force all training columns\n",
        "\n",
        "      event_encoded = pd.DataFrame(\n",
        "          mlb_event.transform(processed[\"Event\"]),\n",
        "          columns=mlb_event.classes_,\n",
        "          index=processed.index\n",
        "      ).reindex(columns=mlb_event.classes_, fill_value=0)  # Force all training columns\n",
        "\n",
        "      # Prepare raw features for ColumnTransformer\n",
        "      features = pd.concat([\n",
        "          processed[[\"Meal Type\", \"Dessert\", \"Menu Avg Leftovers\"]],\n",
        "          menu_encoded,\n",
        "          event_encoded\n",
        "      ], axis=1)\n",
        "\n",
        "      # Apply the preprocessor (includes OneHotEncoder for Meal Type/Dessert)\n",
        "      aligned_features = preprocessor.transform(features)\n",
        "\n",
        "      # Predict totals\n",
        "      totals = np.expm1(voting_regressor.predict(aligned_features))\n",
        "\n",
        "      return totals\n",
        "\n",
        "    \"\"\"Predict total and distribute to items without historical averages\"\"\"\n",
        "    # Load components\n",
        "    model = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/voting_regressor.pkl\")\n",
        "    preprocessor = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/preprocessor.pkl\")\n",
        "    mlb_menu = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_menu.pkl\")\n",
        "    mlb_event = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_event.pkl\")\n",
        "    # Preprocessing\n",
        "    processed = new_meals.copy()\n",
        "    processed[\"Menu\"] = processed[\"Menu\"].str.split(\",\")\n",
        "    processed[\"Event\"] = processed[\"Event\"].str.split(\",\")\n",
        "\n",
        "    # Keep all input items (even unseen ones)\n",
        "    all_items = list(set(item for sublist in processed[\"Menu\"] for item in sublist))\n",
        "\n",
        "    # Encode features\n",
        "    menu_encoded = pd.DataFrame(\n",
        "        mlb_menu.transform(processed[\"Menu\"]),\n",
        "        columns=mlb_menu.classes_,\n",
        "        index=processed.index\n",
        "    ).reindex(columns=all_items, fill_value=0)\n",
        "\n",
        "    event_encoded = pd.DataFrame(\n",
        "        mlb_event.transform(processed[\"Event\"]),\n",
        "        columns=mlb_event.classes_,\n",
        "        index=processed.index\n",
        "    )\n",
        "\n",
        "    # Prepare features\n",
        "    features = pd.concat([\n",
        "        processed[[\"Meal Type\", \"Dessert\"]],\n",
        "        menu_encoded,\n",
        "        event_encoded\n",
        "    ], axis=1)\n",
        "\n",
        "    # Align columns\n",
        "    aligned_features = features.reindex(columns=preprocessor.get_feature_names_out(), fill_value=0)\n",
        "\n",
        "    # Predict totals\n",
        "    totals = predict_total_leftovers(new_meals)\n",
        "\n",
        "        # After preprocessing in predict_new_meals():\n",
        "    #print(\"Aligned Features Sample:\\n\", aligned_features.head())\n",
        "    #print(\"Feature Means:\\n\", aligned_features.mean(axis=0))\n",
        "\n",
        "\n",
        "    # Distribute using NNLS\n",
        "    nmf, H = train_matrix_factorization(data_subset, mlb_menu)\n",
        "    individual_preds = distribute_leftovers(totals, menu_encoded.values, H)\n",
        "    # Save NMF model\n",
        "    joblib.dump(nmf, \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/nmf_model.pkl\")\n",
        "\n",
        "    # Also save the item factors matrix H\n",
        "    joblib.dump(nmf.components_, \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/nmf_components.pkl\")\n",
        "\n",
        "    # Add return statement\n",
        "    return pd.DataFrame(individual_preds, columns=all_items), totals\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CkaDiT2I8lAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§ª Example Usage\n",
        "test_meals = pd.DataFrame([\n",
        "    {\n",
        "        \"Meal Type\": \"B\",\n",
        "        \"Menu\": \"ë² ì´ì»¨ë¶ˆê³ ê¸°ì¹˜ì¦ˆë²„ê±°,ë‹­ë‹¤ë¦¬í›„ë¼ì´ë“œ,ì¹˜í‚¨ë¬´ë‚˜ìµ¸ì†ŒìŠ¤,í‘¸ì‹¤ë¦¬ìƒëŸ¬ë“œ,ì‹œë¦¬ì–¼\",\n",
        "        \"Dessert\": \"1\",\n",
        "        \"Event\": \"3\"\n",
        "    }\n",
        "])\n",
        "individual, total = predict_new_meals(test_meals)\n",
        "print(f\"Total Prediction: {total[0]:.2f}\")\n",
        "print(\"Individual Contributions:\")\n",
        "print(individual.loc[:, individual.iloc[0] > 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg6WlcFW8l-r",
        "outputId": "98cbcf64-30fc-4c71-9f89-98d16db259fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Prediction: 83.12\n",
            "Individual Contributions:\n",
            "   ë² ì´ì»¨ë¶ˆê³ ê¸°ì¹˜ì¦ˆë²„ê±°  ë‹­ë‹¤ë¦¬í›„ë¼ì´ë“œ  í‘¸ì‹¤ë¦¬ìƒëŸ¬ë“œ  ì‹œë¦¬ì–¼  ì¹˜í‚¨ë¬´ë‚˜ìµ¸ì†ŒìŠ¤\n",
            "0          20       14      14   19       14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error Bound"
      ],
      "metadata": {
        "id": "katmK7RH8nxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmf = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/nmf_model.pkl\")"
      ],
      "metadata": {
        "id": "awFq-VBe8nHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import nnls\n",
        "import joblib\n",
        "\n",
        "# Load all components\n",
        "nmf = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/nmf_model.pkl\")\n",
        "voting_regressor = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/voting_regressor.pkl\")\n",
        "preprocessor = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/preprocessor.pkl\")\n",
        "mlb_menu = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_event.pkl\")\n",
        "\n",
        "# Load dataset with true leftovers\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([item.strip() for item in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([item.strip() for item in x.split(\",\")]))\n",
        "\n",
        "\n",
        "def predict_total_leftovers(new_meals):\n",
        "    processed = new_meals.copy()\n",
        "    processed[\"Menu\"] = processed[\"Menu\"].str.split(\",\").apply(lambda x: [i.strip() for i in x])\n",
        "    processed[\"Event\"] = processed[\"Event\"].str.split(\",\").apply(lambda x: [i.strip() for i in x])\n",
        "\n",
        "    # Compute Menu Avg Leftovers from historical dictionary\n",
        "    menu_avg_leftovers = data.explode(\"Menu\").groupby(\"Menu\")[\"leftovers\"].mean().to_dict()\n",
        "    processed[\"Menu Avg Leftovers\"] = processed[\"Menu\"].apply(\n",
        "        lambda items: np.mean([menu_avg_leftovers.get(item, 0) for item in items])\n",
        "    )\n",
        "\n",
        "    # Encoding\n",
        "    menu_encoded = pd.DataFrame(\n",
        "        mlb_menu.transform(processed[\"Menu\"]),\n",
        "        columns=mlb_menu.classes_,\n",
        "        index=processed.index\n",
        "    )\n",
        "    event_encoded = pd.DataFrame(\n",
        "        mlb_event.transform(processed[\"Event\"]),\n",
        "        columns=mlb_event.classes_,\n",
        "        index=processed.index\n",
        "    )\n",
        "\n",
        "    # Concatenate features\n",
        "    features = pd.concat([\n",
        "        processed[[\"Meal Type\", \"Dessert\", \"Menu Avg Leftovers\"]],\n",
        "        menu_encoded,\n",
        "        event_encoded\n",
        "    ], axis=1)\n",
        "\n",
        "    X_input = preprocessor.transform(features)\n",
        "    total_preds = np.expm1(voting_regressor.predict(X_input))\n",
        "\n",
        "    return total_preds[0], menu_encoded, processed\n",
        "\n",
        "\n",
        "def predict_with_bounds_and_theory(new_meals):\n",
        "    total_pred, menu_encoded, processed = predict_total_leftovers(new_meals)\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "    if not active_idx.size:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    H_active = H[:, active_idx]  # shape: (r, |active|)\n",
        "    cond_H = np.linalg.cond(H_active)  # from paper: Îº(H)\n",
        "\n",
        "    # Compute NNLS weights\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    # Get total_true by matching record\n",
        "    matched = data[\n",
        "        (data[\"Meal Type\"] == processed[\"Meal Type\"].iloc[0]) &\n",
        "        (data[\"Dessert\"] == processed[\"Dessert\"].iloc[0]) &\n",
        "        (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(processed[\"Menu\"].iloc[0]))) &\n",
        "        (data[\"Event\"].apply(lambda x: sorted(x) == sorted(processed[\"Event\"].iloc[0])))\n",
        "    ]\n",
        "    if not matched.empty:\n",
        "        total_true = matched[\"leftovers\"].values[0]\n",
        "    else:\n",
        "        total_true = total_pred  # fallback: assume perfect match\n",
        "\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    # Estimate e_i from full reconstruction error\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    #recon_error_vector = np.abs(V - V_hat).mean(axis=0)  # per-item NMF error\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)  # per-item NMF error\n",
        "\n",
        "    # Final bound from theory: |x_i - x*_i| <= Îº(H) * |T_hat - T*| + |e_i|\n",
        "    results = []\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "        results.append({\n",
        "            \"item\": mlb_menu.classes_[idx],\n",
        "            \"prediction\": pred,\n",
        "            \"margin\": bound,\n",
        "            \"percent_error\": (bound / (pred + 1e-8)) * 100\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KU1NtfWY8pzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Example usage\n",
        "test_meals = pd.DataFrame([{\n",
        "    \"Meal Type\": \"D\",\n",
        "    \"Menu\": \"ì˜ì–‘ë°¥,ì½©ë‚˜ë¬¼êµ­,ë¹„ì—”ë‚˜ì†Œì‹œì§€ì•¼ì±„ë³¶ìŒ,ë¼ì§€ê³ ê¸°ê°ìì¡°ë¦¼\",\n",
        "    \"Dessert\": \"0\",\n",
        "    \"Event\": \"3\"\n",
        "}])\n",
        "\n",
        "result_df = predict_with_bounds_and_theory(test_meals)\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in5k7LsB8qmY",
        "outputId": "761e49de-3eee-4ce6-dcd3-818b70276165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         item  prediction    margin  percent_error\n",
            "0    ë¼ì§€ê³ ê¸°ê°ìì¡°ë¦¼   56.617107  0.545452       0.963404\n",
            "1  ë¹„ì—”ë‚˜ì†Œì‹œì§€ì•¼ì±„ë³¶ìŒ   20.511335  0.500000       2.437677\n",
            "2         ì˜ì–‘ë°¥   10.086863  0.000068       0.000676\n",
            "3        ì½©ë‚˜ë¬¼êµ­   29.822491  0.500930       1.679705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program"
      ],
      "metadata": {
        "id": "V9k7RpGJ9B0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ë§¨ ìœ„ library install í•˜ê¸°\n",
        "2. ì„¸ì…˜ ì¬ì‹œì‘í•˜ê¸°"
      ],
      "metadata": {
        "id": "6aZmXPvdbFyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict_leftovers_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Load models and encoders\n",
        "best_pipeline = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/final_pipeline.pkl\")\n",
        "menu_avg_leftovers = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/menu_avg_leftovers.pkl\")\n",
        "mlb_menu = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_event.pkl\")\n",
        "nmf = joblib.load(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/nmf_model.pkl\")\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "\n",
        "DESSERT_MAP = {\"ì—†ìŒ\": \"0\", \"ìœ ì œí’ˆ\": \"1\", \"ê³¼ì¼\": \"2\", \"ê³¼ì¼í‘¸ë”©\": \"3\", \"ì´ì˜¨ìŒë£Œ/ ì—ì´ë“œ/ íƒ„ì‚°\": \"4\", \"í•«ë°”\": \"5\", \"ë§ˆì¹´ë¡±/ ì´ˆì½œë¦¿/ ì—ë„ˆì§€ë°”\": \"6\"}\n",
        "EVENT_MAP = {\"ì£¼ë§, ê³µíœ´ì¼\": \"1\", \"ì£¼ì¤‘\": \"0\", \"ìœ ê²©\": \"4\", \"ì¤‘ëŒ€ ì „ìˆ í›ˆë ¨ ë° ê¸°ë³¸ í›ˆë ¨\": \"3\"}\n",
        "MEAL_TYPE_MAP = {\"ì•„ì¹¨\": \"A\", \"ì ì‹¬\": \"B\", \"ì €ë…\": \"C\", \"ë¸ŒëŸ°ì¹˜\": \"D\"}\n",
        "\n",
        "def predict_leftovers(meal_type, menu_items, dessert, event):\n",
        "    menu_items_list = [i.strip() for i in menu_items.split(\",\")]\n",
        "    known_menu_items = [item for item in menu_items_list if item in menu_avg_leftovers]\n",
        "    if not known_menu_items:\n",
        "        st.error(\"Error: Unknown menu items.\")\n",
        "        return {}\n",
        "\n",
        "    avg_leftovers = np.mean([menu_avg_leftovers[item] for item in known_menu_items])\n",
        "    raw_input = pd.DataFrame({\n",
        "        'Meal Type': [meal_type],\n",
        "        'Dessert': [dessert],\n",
        "        'Menu Avg Leftovers': [avg_leftovers]\n",
        "    })\n",
        "\n",
        "    menu_encoded = pd.DataFrame(mlb_menu.transform([menu_items_list]), columns=mlb_menu.classes_)\n",
        "    event_encoded = pd.DataFrame(mlb_event.transform([[event]]), columns=mlb_event.classes_)\n",
        "    input_df = pd.concat([raw_input, menu_encoded, event_encoded], axis=1)\n",
        "    input_np = best_pipeline.named_steps['preprocessor'].transform(input_df)\n",
        "\n",
        "    total_pred = best_pipeline.named_steps['regressor'].predict(input_np)[0]\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "    H_active = H[:, active_idx]\n",
        "    cond_H = np.linalg.cond(H_active)\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    matched = data[(data[\"Meal Type\"] == meal_type) &\n",
        "                   (data[\"Dessert\"] == dessert) &\n",
        "                   (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(menu_items_list))) &\n",
        "                   (data[\"Event\"].apply(lambda x: sorted(x) == sorted([event])))]\n",
        "\n",
        "    total_true = matched[\"leftovers\"].values[0] if not matched.empty else total_pred\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)\n",
        "\n",
        "    predictions = {}\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        item = mlb_menu.classes_[idx]\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "\n",
        "        # âœ… boundê°€ NaN ë˜ëŠ” infì´ë©´ 0ìœ¼ë¡œ ê³ ì •\n",
        "        if not np.isfinite(bound):\n",
        "            bound = 0.0\n",
        "\n",
        "        percent = (bound / (pred + 1e-8)) * 100\n",
        "        if not np.isfinite(percent) or percent > 100:\n",
        "            percent = 0.0\n",
        "\n",
        "        predictions[item] = f\"{pred:.1f} Â± {bound:.1f} ({percent:.1f}%)\"\n",
        "\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# ğŸ¨ Streamlit UI with Korean Labels\n",
        "st.title(\"ğŸ› êµ° ê¸‰ì‹ ì”ë°˜ ì˜ˆì¸¡ í”„ë¡œê·¸ë¨\")\n",
        "st.markdown(\"ë©”ë‰´ ì •ë³´ë¥¼ ì…ë ¥í•˜ë©´ ì˜ˆìƒ ì”ë°˜ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "meal_type_korean = st.selectbox(\"ğŸ½ï¸ ì‹ì‚¬ ì¢…ë¥˜ ì„ íƒ\", list(MEAL_TYPE_MAP.keys()))\n",
        "meal_type = MEAL_TYPE_MAP[meal_type_korean]\n",
        "\n",
        "menu_items = st.text_input(\"ğŸ² ë©”ë‰´ í•­ëª© ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)\", \"ì˜ì–‘ë°¥,ì½©ë‚˜ë¬¼êµ­,ë¹„ì—”ë‚˜ì†Œì‹œì§€ì•¼ì±„ë³¶ìŒ,ë¼ì§€ê³ ê¸°ê°ìì¡°ë¦¼\")\n",
        "\n",
        "dessert_korean = st.selectbox(\"ğŸ° ë””ì €íŠ¸ ì„ íƒ\", list(DESSERT_MAP.keys()))\n",
        "dessert = DESSERT_MAP[dessert_korean]\n",
        "\n",
        "event_korean = st.selectbox(\"ğŸ¯ í–‰ì‚¬ ì„ íƒ\", list(EVENT_MAP.keys()))\n",
        "event = EVENT_MAP[event_korean]\n",
        "\n",
        "# âœ… NEW: Enter number of people\n",
        "num_people = st.number_input(\"ğŸ‘¥ ì‹ì‚¬ ì¸ì› ìˆ˜\", min_value=1, value=100)\n",
        "\n",
        "\n",
        "# âœ… Predict button\n",
        "if st.button(\"ğŸ§® ì˜ˆì¸¡í•˜ê¸°\"):\n",
        "    with st.spinner(\"ê³„ì‚° ì¤‘...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people / 1000:.2f} kg Â± {float(v.split()[2]) * num_people / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(\"âœ… ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
        "            st.write(\"### ğŸ½ï¸ ì˜ˆìƒ ì”ë°˜ëŸ‰ (ê° ë©”ë‰´ë³„)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "# ğŸ”§ Additional Percentage Slider and Button\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"ğŸ”§ íŠ¹ì • ë¹„ìœ¨ë¡œ ì”ë°˜ëŸ‰ ê³„ì‚°\")\n",
        "\n",
        "percentage = st.slider(\"ğŸ”§ ì˜ˆì¸¡ ì”ë°˜ì˜ ëª‡ í¼ì„¼íŠ¸ë¥¼ ë°˜í™˜í• ê¹Œìš”?\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"ğŸ”„ íŠ¹ì • ë¹„ìœ¨ë¡œ ì”ë°˜ ê³„ì‚°í•˜ê¸°\"):\n",
        "    with st.spinner(\"ê³„ì‚° ì¤‘...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people * (percentage / 100) / 1000:.2f} kg Â± {float(v.split()[2]) * num_people * (percentage / 100) / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ! ({percentage}% ê¸°ì¤€)\")\n",
        "            st.write(f\"### ğŸ½ï¸ ì˜ˆìƒ ì”ë°˜ëŸ‰ - {percentage}% ê¸°ì¤€ (ê° ë©”ë‰´ë³„)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n32dLCec9teA",
        "outputId": "5e5c93e2-7a33-4977-c3a5-113f61c59d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting predict_leftovers_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run predict_leftovers_app.py --server.port 8501 &\n",
        "!ngrok config add-authtoken 2uO7qKTmyrri0YTq05KzyH0BWBW_7RhnrTVfK3JfGVzfHyCCq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnLHKty69EgP",
        "outputId": "6aa0ed17-2d81-4f4d-9cc1-5b1aa0afd2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill any previous ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a new ngrok tunnel (fixing the API format issue)\n",
        "public_url = ngrok.connect(8501, \"http\")  # âœ… FIX: Use `http` instead of `port`\n",
        "\n",
        "print(\"ğŸš€ Streamlit App is running at:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RpgZvdZ9FZP",
        "outputId": "652a7a46-dc26-44c8-d30a-ee6fa96437ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Streamlit App is running at: NgrokTunnel: \"https://0db1-35-236-163-7.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyinstaller"
      ],
      "metadata": {
        "id": "reEqB3ZN-R9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326d22f1-52ab-49f2-9639-018a4b1038e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyinstaller\n",
            "  Downloading pyinstaller-6.13.0-py3-none-manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: setuptools>=42.0.0 in /usr/local/lib/python3.11/dist-packages (from pyinstaller) (75.2.0)\n",
            "Collecting altgraph (from pyinstaller)\n",
            "  Downloading altgraph-0.17.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pyinstaller-hooks-contrib>=2025.2 (from pyinstaller)\n",
            "  Downloading pyinstaller_hooks_contrib-2025.3-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from pyinstaller) (24.2)\n",
            "Downloading pyinstaller-6.13.0-py3-none-manylinux2014_x86_64.whl (721 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m721.0/721.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyinstaller_hooks_contrib-2025.3-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m434.3/434.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading altgraph-0.17.4-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: altgraph, pyinstaller-hooks-contrib, pyinstaller\n",
            "Successfully installed altgraph-0.17.4 pyinstaller-6.13.0 pyinstaller-hooks-contrib-2025.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# ì••ì¶•í•  í´ë” ìƒì„±\n",
        "os.makedirs(\"/content/offline_app\", exist_ok=True)\n",
        "\n",
        "# âœ… Streamlit ì•± ì½”ë“œ ì €ì¥\n",
        "with open(\"/content/offline_app/predict_leftovers_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\n",
        "\n",
        "\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Load models and encoders\n",
        "best_pipeline = joblib.load(\"final_pipeline.pkl\")\n",
        "menu_avg_leftovers = joblib.load(\"menu_avg_leftovers.pkl\")\n",
        "mlb_menu = joblib.load(\"mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"mlb_event.pkl\")\n",
        "nmf = joblib.load(\"nmf_model.pkl\")\n",
        "data = pd.read_excel(\"current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "\n",
        "DESSERT_MAP = {\"ì—†ìŒ\": \"0\", \"ìœ ì œí’ˆ\": \"1\", \"ê³¼ì¼\": \"2\", \"ê³¼ì¼í‘¸ë”©\": \"3\", \"ì´ì˜¨ìŒë£Œ/ ì—ì´ë“œ/ íƒ„ì‚°\": \"4\", \"í•«ë°”\": \"5\", \"ë§ˆì¹´ë¡±/ ì´ˆì½œë¦¿/ ì—ë„ˆì§€ë°”\": \"6\"}\n",
        "EVENT_MAP = {\"ì£¼ë§, ê³µíœ´ì¼\": \"1\", \"ì£¼ì¤‘\": \"0\", \"ìœ ê²©\": \"4\", \"ì¤‘ëŒ€ ì „ìˆ í›ˆë ¨ ë° ê¸°ë³¸ í›ˆë ¨\": \"3\"}\n",
        "MEAL_TYPE_MAP = {\"ì•„ì¹¨\": \"A\", \"ì ì‹¬\": \"B\", \"ì €ë…\": \"C\", \"ë¸ŒëŸ°ì¹˜\": \"D\"}\n",
        "\n",
        "def predict_leftovers(meal_type, menu_items, dessert, event):\n",
        "    menu_items_list = [i.strip() for i in menu_items.split(\",\")]\n",
        "    known_menu_items = [item for item in menu_items_list if item in menu_avg_leftovers]\n",
        "    if not known_menu_items:\n",
        "        st.error(\"Error: Unknown menu items.\")\n",
        "        return {}\n",
        "\n",
        "    avg_leftovers = np.mean([menu_avg_leftovers[item] for item in known_menu_items])\n",
        "    raw_input = pd.DataFrame({\n",
        "        'Meal Type': [meal_type],\n",
        "        'Dessert': [dessert],\n",
        "        'Menu Avg Leftovers': [avg_leftovers]\n",
        "    })\n",
        "\n",
        "    menu_encoded = pd.DataFrame(mlb_menu.transform([menu_items_list]), columns=mlb_menu.classes_)\n",
        "    event_encoded = pd.DataFrame(mlb_event.transform([[event]]), columns=mlb_event.classes_)\n",
        "    input_df = pd.concat([raw_input, menu_encoded, event_encoded], axis=1)\n",
        "    input_np = best_pipeline.named_steps['preprocessor'].transform(input_df)\n",
        "\n",
        "    total_pred = best_pipeline.named_steps['regressor'].predict(input_np)[0]\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "    H_active = H[:, active_idx]\n",
        "    cond_H = np.linalg.cond(H_active)\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    matched = data[(data[\"Meal Type\"] == meal_type) &\n",
        "                   (data[\"Dessert\"] == dessert) &\n",
        "                   (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(menu_items_list))) &\n",
        "                   (data[\"Event\"].apply(lambda x: sorted(x) == sorted([event])))]\n",
        "\n",
        "    total_true = matched[\"leftovers\"].values[0] if not matched.empty else total_pred\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)\n",
        "\n",
        "    predictions = {}\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        item = mlb_menu.classes_[idx]\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "\n",
        "        # âœ… boundê°€ NaN ë˜ëŠ” infì´ë©´ 0ìœ¼ë¡œ ê³ ì •\n",
        "        if not np.isfinite(bound):\n",
        "            bound = 0.0\n",
        "\n",
        "        percent = (bound / (pred + 1e-8)) * 100\n",
        "        if not np.isfinite(percent) or percent > 100:\n",
        "            percent = 0.0\n",
        "\n",
        "        predictions[item] = f\"{pred:.1f} Â± {bound:.1f} ({percent:.1f}%)\"\n",
        "\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# ğŸ¨ Streamlit UI with Korean Labels\n",
        "st.title(\"ğŸ› êµ° ê¸‰ì‹ ì”ë°˜ ì˜ˆì¸¡ í”„ë¡œê·¸ë¨\")\n",
        "st.markdown(\"ë©”ë‰´ ì •ë³´ë¥¼ ì…ë ¥í•˜ë©´ ì˜ˆìƒ ì”ë°˜ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "meal_type_korean = st.selectbox(\"ğŸ½ï¸ ì‹ì‚¬ ì¢…ë¥˜ ì„ íƒ\", list(MEAL_TYPE_MAP.keys()))\n",
        "meal_type = MEAL_TYPE_MAP[meal_type_korean]\n",
        "\n",
        "menu_items = st.text_input(\"ğŸ² ë©”ë‰´ í•­ëª© ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)\", \"ì˜ì–‘ë°¥,ì½©ë‚˜ë¬¼êµ­,ë¹„ì—”ë‚˜ì†Œì‹œì§€ì•¼ì±„ë³¶ìŒ,ë¼ì§€ê³ ê¸°ê°ìì¡°ë¦¼\")\n",
        "\n",
        "dessert_korean = st.selectbox(\"ğŸ° ë””ì €íŠ¸ ì„ íƒ\", list(DESSERT_MAP.keys()))\n",
        "dessert = DESSERT_MAP[dessert_korean]\n",
        "\n",
        "event_korean = st.selectbox(\"ğŸ¯ í–‰ì‚¬ ì„ íƒ\", list(EVENT_MAP.keys()))\n",
        "event = EVENT_MAP[event_korean]\n",
        "\n",
        "# âœ… NEW: Enter number of people\n",
        "num_people = st.number_input(\"ğŸ‘¥ ì‹ì‚¬ ì¸ì› ìˆ˜\", min_value=1, value=100)\n",
        "\n",
        "\n",
        "# âœ… Predict button\n",
        "if st.button(\"ğŸ§® ì˜ˆì¸¡í•˜ê¸°\"):\n",
        "    with st.spinner(\"ê³„ì‚° ì¤‘...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people / 1000:.2f} kg Â± {float(v.split()[2]) * num_people / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(\"âœ… ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
        "            st.write(\"### ğŸ½ï¸ ì˜ˆìƒ ì”ë°˜ëŸ‰ (ê° ë©”ë‰´ë³„)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "# ğŸ”§ Additional Percentage Slider and Button\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"ğŸ”§ íŠ¹ì • ë¹„ìœ¨ë¡œ ì”ë°˜ëŸ‰ ê³„ì‚°\")\n",
        "\n",
        "percentage = st.slider(\"ğŸ”§ ì˜ˆì¸¡ ì”ë°˜ì˜ ëª‡ í¼ì„¼íŠ¸ë¥¼ ë°˜í™˜í• ê¹Œìš”?\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"ğŸ”„ íŠ¹ì • ë¹„ìœ¨ë¡œ ì”ë°˜ ê³„ì‚°í•˜ê¸°\"):\n",
        "    with st.spinner(\"ê³„ì‚° ì¤‘...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people * (percentage / 100) / 1000:.2f} kg Â± {float(v.split()[2]) * num_people * (percentage / 100) / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ! ({percentage}% ê¸°ì¤€)\")\n",
        "            st.write(f\"### ğŸ½ï¸ ì˜ˆìƒ ì”ë°˜ëŸ‰ - {percentage}% ê¸°ì¤€ (ê° ë©”ë‰´ë³„)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# âœ… requirements.txt ìƒì„±\n",
        "with open(\"/content/offline_app/requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "streamlit==1.32.2\n",
        "scikit-learn==1.4.2\n",
        "numpy==1.26.4\n",
        "pandas==2.2.2\n",
        "joblib==1.4.0\n",
        "openpyxl==3.1.2\n",
        "scipy==1.13.0\n",
        "xgboost==2.0.3\n",
        "catboost==1.2.7\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# âœ… ëª¨ë¸ê³¼ ë°ì´í„° ë³µì‚¬\n",
        "file_paths = [\n",
        "    \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/final_pipeline.pkl\",\n",
        "    \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/menu_avg_leftovers.pkl\",\n",
        "    \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_menu.pkl\",\n",
        "    \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_event.pkl\",\n",
        "    \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/nmf_model.pkl\",\n",
        "    \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/current_processed_menu_data.xlsx\",\n",
        "]\n",
        "\n",
        "for path in file_paths:\n",
        "    shutil.copy(path, \"/content/offline_app\")\n",
        "\n",
        "# âœ… ì••ì¶•í•˜ê¸°\n",
        "with ZipFile(\"/content/predict_leftovers_offline_package.zip\", \"w\") as zipf:\n",
        "    for fname in os.listdir(\"/content/offline_app\"):\n",
        "        zipf.write(os.path.join(\"/content/offline_app\", fname), arcname=fname)\n"
      ],
      "metadata": {
        "id": "ZagIwio9ogIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/offline_app.zip /content/offline_app\n",
        "from google.colab import files\n",
        "files.download(\"/content/offline_app.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "p1hJVefKrli_",
        "outputId": "367f93f2-03db-4b5c-f0db-eae37cecf1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/offline_app/ (stored 0%)\n",
            "updating: content/offline_app/mlb_menu.pkl (deflated 63%)\n",
            "updating: content/offline_app/requirements.txt (deflated 24%)\n",
            "updating: content/offline_app/predict_leftovers_app.py (deflated 60%)\n",
            "updating: content/offline_app/final_pipeline.pkl (deflated 76%)\n",
            "updating: content/offline_app/current_processed_menu_data.xlsx (deflated 8%)\n",
            "updating: content/offline_app/mlb_event.pkl (deflated 29%)\n",
            "updating: content/offline_app/nmf_model.pkl (deflated 99%)\n",
            "updating: content/offline_app/menu_avg_leftovers.pkl (deflated 66%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d41fc98-7406-4113-a98b-ad8a5b83dbef\", \"offline_app.zip\", 849433)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Load models and encoders\n",
        "best_pipeline = joblib.load(\"final_pipeline.pkl\")\n",
        "menu_avg_leftovers = joblib.load(\"menu_avg_leftovers.pkl\")\n",
        "mlb_menu = joblib.load(\"mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"mlb_event.pkl\")\n",
        "nmf = joblib.load(\"nmf_model.pkl\")\n",
        "data = pd.read_excel(\"current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "\n",
        "DESSERT_MAP = {\"ì—†ìŒ\": \"0\", \"ìœ ì œí’ˆ\": \"1\", \"ê³¼ì¼\": \"2\", \"ê³¼ì¼í‘¸ë”©\": \"3\", \"ì´ì˜¨ìŒë£Œ/ ì—ì´ë“œ/ íƒ„ì‚°\": \"4\", \"í•«ë°”\": \"5\", \"ë§ˆì¹´ë¡±/ ì´ˆì½œë¦¿/ ì—ë„ˆì§€ë°”\": \"6\"}\n",
        "EVENT_MAP = {\"ì£¼ë§, ê³µíœ´ì¼\": \"1\", \"ì£¼ì¤‘\": \"0\", \"ìœ ê²©\": \"4\", \"ì¤‘ëŒ€ ì „ìˆ í›ˆë ¨ ë° ê¸°ë³¸ í›ˆë ¨\": \"3\"}\n",
        "MEAL_TYPE_MAP = {\"ì•„ì¹¨\": \"A\", \"ì ì‹¬\": \"B\", \"ì €ë…\": \"C\", \"ë¸ŒëŸ°ì¹˜\": \"D\"}\n",
        "\n",
        "def predict_leftovers(meal_type, menu_items, dessert, event):\n",
        "    menu_items_list = [i.strip() for i in menu_items.split(\",\")]\n",
        "    known_menu_items = [item for item in menu_items_list if item in menu_avg_leftovers]\n",
        "    if not known_menu_items:\n",
        "        st.error(\"Error: Unknown menu items.\")\n",
        "        return {}\n",
        "\n",
        "    avg_leftovers = np.mean([menu_avg_leftovers[item] for item in known_menu_items])\n",
        "    raw_input = pd.DataFrame({\n",
        "        'Meal Type': [meal_type],\n",
        "        'Dessert': [dessert],\n",
        "        'Menu Avg Leftovers': [avg_leftovers]\n",
        "    })\n",
        "\n",
        "    menu_encoded = pd.DataFrame(mlb_menu.transform([menu_items_list]), columns=mlb_menu.classes_)\n",
        "    event_encoded = pd.DataFrame(mlb_event.transform([[event]]), columns=mlb_event.classes_)\n",
        "    input_df = pd.concat([raw_input, menu_encoded, event_encoded], axis=1)\n",
        "    input_np = best_pipeline.named_steps['preprocessor'].transform(input_df)\n",
        "\n",
        "    total_pred = best_pipeline.named_steps['regressor'].predict(input_np)[0]\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "    H_active = H[:, active_idx]\n",
        "    cond_H = np.linalg.cond(H_active)\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    matched = data[(data[\"Meal Type\"] == meal_type) &\n",
        "                   (data[\"Dessert\"] == dessert) &\n",
        "                   (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(menu_items_list))) &\n",
        "                   (data[\"Event\"].apply(lambda x: sorted(x) == sorted([event])))]\n",
        "\n",
        "    total_true = matched[\"leftovers\"].values[0] if not matched.empty else total_pred\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)\n",
        "\n",
        "    predictions = {}\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        item = mlb_menu.classes_[idx]\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "\n",
        "        # âœ… boundê°€ NaN ë˜ëŠ” infì´ë©´ 0ìœ¼ë¡œ ê³ ì •\n",
        "        if not np.isfinite(bound):\n",
        "            bound = 0.0\n",
        "\n",
        "        percent = (bound / (pred + 1e-8)) * 100\n",
        "        if not np.isfinite(percent) or percent > 100:\n",
        "            percent = 0.0\n",
        "\n",
        "        predictions[item] = f\"{pred:.1f} Â± {bound:.1f} ({percent:.1f}%)\"\n",
        "\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# ğŸ¨ Streamlit UI with Korean Labels\n",
        "st.title(\"ğŸ› êµ° ê¸‰ì‹ ì”ë°˜ ì˜ˆì¸¡ í”„ë¡œê·¸ë¨\")\n",
        "st.markdown(\"ë©”ë‰´ ì •ë³´ë¥¼ ì…ë ¥í•˜ë©´ ì˜ˆìƒ ì”ë°˜ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "meal_type_korean = st.selectbox(\"ğŸ½ï¸ ì‹ì‚¬ ì¢…ë¥˜ ì„ íƒ\", list(MEAL_TYPE_MAP.keys()))\n",
        "meal_type = MEAL_TYPE_MAP[meal_type_korean]\n",
        "\n",
        "menu_items = st.text_input(\"ğŸ² ë©”ë‰´ í•­ëª© ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)\", \"ì˜ì–‘ë°¥,ì½©ë‚˜ë¬¼êµ­,ë¹„ì—”ë‚˜ì†Œì‹œì§€ì•¼ì±„ë³¶ìŒ,ë¼ì§€ê³ ê¸°ê°ìì¡°ë¦¼\")\n",
        "\n",
        "dessert_korean = st.selectbox(\"ğŸ° ë””ì €íŠ¸ ì„ íƒ\", list(DESSERT_MAP.keys()))\n",
        "dessert = DESSERT_MAP[dessert_korean]\n",
        "\n",
        "event_korean = st.selectbox(\"ğŸ¯ í–‰ì‚¬ ì„ íƒ\", list(EVENT_MAP.keys()))\n",
        "event = EVENT_MAP[event_korean]\n",
        "\n",
        "# âœ… NEW: Enter number of people\n",
        "num_people = st.number_input(\"ğŸ‘¥ ì‹ì‚¬ ì¸ì› ìˆ˜\", min_value=1, value=100)\n",
        "\n",
        "\n",
        "# âœ… Predict button\n",
        "if st.button(\"ğŸ§® ì˜ˆì¸¡í•˜ê¸°\"):\n",
        "    with st.spinner(\"ê³„ì‚° ì¤‘...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people / 1000:.2f} kg Â± {float(v.split()[2]) * num_people / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(\"âœ… ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
        "            st.write(\"### ğŸ½ï¸ ì˜ˆìƒ ì”ë°˜ëŸ‰ (ê° ë©”ë‰´ë³„)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "# ğŸ”§ Additional Percentage Slider and Button\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"ğŸ”§ íŠ¹ì • ë¹„ìœ¨ë¡œ ì”ë°˜ëŸ‰ ê³„ì‚°\")\n",
        "\n",
        "percentage = st.slider(\"ğŸ”§ ì˜ˆì¸¡ ì”ë°˜ì˜ ëª‡ í¼ì„¼íŠ¸ë¥¼ ë°˜í™˜í• ê¹Œìš”?\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"ğŸ”„ íŠ¹ì • ë¹„ìœ¨ë¡œ ì”ë°˜ ê³„ì‚°í•˜ê¸°\"):\n",
        "    with st.spinner(\"ê³„ì‚° ì¤‘...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people * (percentage / 100) / 1000:.2f} kg Â± {float(v.split()[2]) * num_people * (percentage / 100) / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ! ({percentage}% ê¸°ì¤€)\")\n",
        "            st.write(f\"### ğŸ½ï¸ ì˜ˆìƒ ì”ë°˜ëŸ‰ - {percentage}% ê¸°ì¤€ (ê° ë©”ë‰´ë³„)\")\n",
        "            st.json(scaled_predictions)"
      ],
      "metadata": {
        "id": "nBUTYrv5dSfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# ğŸ”§ ì˜¤í”„ë¼ì¸ ì•± í´ë” ì´ˆê¸°í™”\n",
        "base_dir = \"/content/offline_app\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "os.makedirs(f\"{base_dir}/whl\", exist_ok=True)\n",
        "\n",
        "# âœ… í•„ìš”í•œ .whl íŒŒì¼ë“¤ ë‹¤ìš´ë¡œë“œ\n",
        "wheels = {\n",
        "    \"numpy\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/numpy-1.26.4-cp311-cp311-win_amd64.whl\",\n",
        "    \"pandas\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/pandas-2.2.2-cp311-cp311-win_amd64.whl\",\n",
        "    \"scipy\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/scipy-1.13.0-cp311-cp311-win_amd64.whl\",\n",
        "    \"scikit-learn\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/scikit_learn-1.4.2-cp311-cp311-win_amd64.whl\",\n",
        "    \"catboost\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/catboost-1.2.7-cp311-cp311-win_amd64.whl\",\n",
        "    \"xgboost\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/xgboost-2.0.3-cp311-cp311-win_amd64.whl\",\n",
        "    \"openpyxl\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/openpyxl-3.1.2-py3-none-any.whl\",\n",
        "    \"joblib\": \"https://download.lfd.uci.edu/pythonlibs/w4tscw6k/joblib-1.4.0-py3-none-any.whl\",\n",
        "    \"streamlit\": \"https://files.pythonhosted.org/packages/7d/b6/f9e62e508caaa8b5c2c4f6aa4fc6469b1b30761c7fdc6027c07e55d7918d/streamlit-1.32.2-py2.py3-none-any.whl\"\n",
        "}\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "for name, url in wheels.items():\n",
        "    out_path = os.path.join(base_dir, \"whl\", os.path.basename(url))\n",
        "    if not os.path.exists(out_path):\n",
        "        urlretrieve(url, out_path)\n",
        "\n",
        "# âœ… requirements.txt ìƒì„±\n",
        "with open(f\"{base_dir}/requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"numpy\n",
        "pandas\n",
        "scipy\n",
        "scikit-learn\n",
        "catboost\n",
        "xgboost\n",
        "openpyxl\n",
        "joblib\n",
        "streamlit\"\"\")\n",
        "\n",
        "# âœ… start_app.bat ìƒì„±\n",
        "with open(f\"{base_dir}/start_app.bat\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(r\"\"\"@echo off\n",
        "cd /d \"%~dp0\"\n",
        "python -m venv venv\n",
        "call venv\\Scripts\\activate\n",
        "pip install --upgrade pip\n",
        "pip install --no-index --find-links=whl -r requirements.txt\n",
        "streamlit run predict_leftovers_app.py\n",
        "pause\n",
        "\"\"\")\n",
        "\n",
        "with open(\"/content/offline_app/predict_leftovers_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\n",
        "\n",
        "\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Load models and encoders\n",
        "best_pipeline = joblib.load(\"final_pipeline.pkl\")\n",
        "menu_avg_leftovers = joblib.load(\"menu_avg_leftovers.pkl\")\n",
        "mlb_menu = joblib.load(\"mlb_menu.pkl\")\n",
        "mlb_event = joblib.load(\"mlb_event.pkl\")\n",
        "nmf = joblib.load(\"nmf_model.pkl\")\n",
        "data = pd.read_excel(\"current_processed_menu_data.xlsx\")\n",
        "data[\"Menu\"] = data[\"Menu\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "data[\"Event\"] = data[\"Event\"].astype(str).apply(lambda x: sorted([i.strip() for i in x.split(\",\")]))\n",
        "\n",
        "DESSERT_MAP = {\"ì—†ìŒ\": \"0\", \"ìœ ì œí’ˆ\": \"1\", \"ê³¼ì¼\": \"2\", \"ê³¼ì¼í‘¸ë”©\": \"3\", \"ì´ì˜¨ìŒë£Œ/ ì—ì´ë“œ/ íƒ„ì‚°\": \"4\", \"í•«ë°”\": \"5\", \"ë§ˆì¹´ë¡±/ ì´ˆì½œë¦¿/ ì—ë„ˆì§€ë°”\": \"6\"}\n",
        "EVENT_MAP = {\"ì£¼ë§, ê³µíœ´ì¼\": \"1\", \"ì£¼ì¤‘\": \"0\", \"ìœ ê²©\": \"4\", \"ì¤‘ëŒ€ ì „ìˆ í›ˆë ¨ ë° ê¸°ë³¸ í›ˆë ¨\": \"3\"}\n",
        "MEAL_TYPE_MAP = {\"ì•„ì¹¨\": \"A\", \"ì ì‹¬\": \"B\", \"ì €ë…\": \"C\", \"ë¸ŒëŸ°ì¹˜\": \"D\"}\n",
        "\n",
        "def predict_leftovers(meal_type, menu_items, dessert, event):\n",
        "    menu_items_list = [i.strip() for i in menu_items.split(\",\")]\n",
        "    known_menu_items = [item for item in menu_items_list if item in menu_avg_leftovers]\n",
        "    if not known_menu_items:\n",
        "        st.error(\"Error: Unknown menu items.\")\n",
        "        return {}\n",
        "\n",
        "    avg_leftovers = np.mean([menu_avg_leftovers[item] for item in known_menu_items])\n",
        "    raw_input = pd.DataFrame({\n",
        "        'Meal Type': [meal_type],\n",
        "        'Dessert': [dessert],\n",
        "        'Menu Avg Leftovers': [avg_leftovers]\n",
        "    })\n",
        "\n",
        "    menu_encoded = pd.DataFrame(mlb_menu.transform([menu_items_list]), columns=mlb_menu.classes_)\n",
        "    event_encoded = pd.DataFrame(mlb_event.transform([[event]]), columns=mlb_event.classes_)\n",
        "    input_df = pd.concat([raw_input, menu_encoded, event_encoded], axis=1)\n",
        "    input_np = best_pipeline.named_steps['preprocessor'].transform(input_df)\n",
        "\n",
        "    total_pred = best_pipeline.named_steps['regressor'].predict(input_np)[0]\n",
        "    menu_vec = menu_encoded.values[0]\n",
        "    H = nmf.components_\n",
        "    active_idx = np.where(menu_vec > 0)[0]\n",
        "\n",
        "    H_active = H[:, active_idx]\n",
        "    cond_H = np.linalg.cond(H_active)\n",
        "    A = H_active.T @ H_active + 1e-6 * np.eye(len(active_idx))\n",
        "    b = H_active.T @ np.ones(H_active.shape[0]) * total_pred\n",
        "    weights, _ = nnls(A, b)\n",
        "    weights = weights / (weights.sum() + 1e-10) * total_pred\n",
        "\n",
        "    matched = data[(data[\"Meal Type\"] == meal_type) &\n",
        "                   (data[\"Dessert\"] == dessert) &\n",
        "                   (data[\"Menu\"].apply(lambda x: sorted(x) == sorted(menu_items_list))) &\n",
        "                   (data[\"Event\"].apply(lambda x: sorted(x) == sorted([event])))]\n",
        "\n",
        "    total_true = matched[\"leftovers\"].values[0] if not matched.empty else total_pred\n",
        "    delta_T = abs(total_pred - total_true)\n",
        "\n",
        "    V = mlb_menu.transform(data[\"Menu\"])\n",
        "    W = nmf.transform(V)\n",
        "    V_hat = W @ H\n",
        "    recon_error_vector = np.abs(V - V_hat).max(axis=0)\n",
        "\n",
        "    predictions = {}\n",
        "    for i, idx in enumerate(active_idx):\n",
        "        item = mlb_menu.classes_[idx]\n",
        "        pred = weights[i]\n",
        "        e_i = recon_error_vector[idx]\n",
        "        bound = cond_H * delta_T + e_i\n",
        "\n",
        "        # âœ… boundê°€ NaN ë˜ëŠ” infì´ë©´ 0ìœ¼ë¡œ ê³ ì •\n",
        "        if not np.isfinite(bound):\n",
        "            bound = 0.0\n",
        "\n",
        "        percent = (bound / (pred + 1e-8)) * 100\n",
        "        if not np.isfinite(percent) or percent > 100:\n",
        "            percent = 0.0\n",
        "\n",
        "        predictions[item] = f\"{pred:.1f} Â± {bound:.1f} ({percent:.1f}%)\"\n",
        "\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# ğŸ¨ Streamlit UI with Korean Labels\n",
        "st.title(\"ğŸ› êµ° ê¸‰ì‹ ì”ë°˜ ì˜ˆì¸¡ í”„ë¡œê·¸ë¨\")\n",
        "st.markdown(\"ë©”ë‰´ ì •ë³´ë¥¼ ì…ë ¥í•˜ë©´ ì˜ˆìƒ ì”ë°˜ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "meal_type_korean = st.selectbox(\"ğŸ½ï¸ ì‹ì‚¬ ì¢…ë¥˜ ì„ íƒ\", list(MEAL_TYPE_MAP.keys()))\n",
        "meal_type = MEAL_TYPE_MAP[meal_type_korean]\n",
        "\n",
        "menu_items = st.text_input(\"ğŸ² ë©”ë‰´ í•­ëª© ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)\", \"ì˜ì–‘ë°¥,ì½©ë‚˜ë¬¼êµ­,ë¹„ì—”ë‚˜ì†Œì‹œì§€ì•¼ì±„ë³¶ìŒ,ë¼ì§€ê³ ê¸°ê°ìì¡°ë¦¼\")\n",
        "\n",
        "dessert_korean = st.selectbox(\"ğŸ° ë””ì €íŠ¸ ì„ íƒ\", list(DESSERT_MAP.keys()))\n",
        "dessert = DESSERT_MAP[dessert_korean]\n",
        "\n",
        "event_korean = st.selectbox(\"ğŸ¯ í–‰ì‚¬ ì„ íƒ\", list(EVENT_MAP.keys()))\n",
        "event = EVENT_MAP[event_korean]\n",
        "\n",
        "# âœ… NEW: Enter number of people\n",
        "num_people = st.number_input(\"ğŸ‘¥ ì‹ì‚¬ ì¸ì› ìˆ˜\", min_value=1, value=100)\n",
        "\n",
        "\n",
        "# âœ… Predict button\n",
        "if st.button(\"ğŸ§® ì˜ˆì¸¡í•˜ê¸°\"):\n",
        "    with st.spinner(\"ê³„ì‚° ì¤‘...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people / 1000:.2f} kg Â± {float(v.split()[2]) * num_people / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(\"âœ… ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
        "            st.write(\"### ğŸ½ï¸ ì˜ˆìƒ ì”ë°˜ëŸ‰ (ê° ë©”ë‰´ë³„)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "# ğŸ”§ Additional Percentage Slider and Button\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"ğŸ”§ íŠ¹ì • ë¹„ìœ¨ë¡œ ì”ë°˜ëŸ‰ ê³„ì‚°\")\n",
        "\n",
        "percentage = st.slider(\"ğŸ”§ ì˜ˆì¸¡ ì”ë°˜ì˜ ëª‡ í¼ì„¼íŠ¸ë¥¼ ë°˜í™˜í• ê¹Œìš”?\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"ğŸ”„ íŠ¹ì • ë¹„ìœ¨ë¡œ ì”ë°˜ ê³„ì‚°í•˜ê¸°\"):\n",
        "    with st.spinner(\"ê³„ì‚° ì¤‘...\"):\n",
        "        predictions = predict_leftovers(meal_type, menu_items, dessert, event)\n",
        "        if predictions:\n",
        "            scaled_predictions = {\n",
        "                k: f\"{float(v.split()[0]) * num_people * (percentage / 100) / 1000:.2f} kg Â± {float(v.split()[2]) * num_people * (percentage / 100) / 1000:.2f} kg {v.split()[3].replace('((', '(').replace('))', ')')}\"\n",
        "                for k, v in predictions.items()\n",
        "            }\n",
        "            st.success(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ! ({percentage}% ê¸°ì¤€)\")\n",
        "            st.write(f\"### ğŸ½ï¸ ì˜ˆìƒ ì”ë°˜ëŸ‰ - {percentage}% ê¸°ì¤€ (ê° ë©”ë‰´ë³„)\")\n",
        "            st.json(scaled_predictions)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# âœ… ëª¨ë¸ ë° ë°ì´í„° íŒŒì¼ ë³µì‚¬\n",
        "file_paths = {\n",
        "    \"final_pipeline.pkl\": \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/final_pipeline.pkl\",\n",
        "    \"menu_avg_leftovers.pkl\": \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/menu_avg_leftovers.pkl\",\n",
        "    \"mlb_menu.pkl\": \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_menu.pkl\",\n",
        "    \"mlb_event.pkl\": \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/mlb_event.pkl\",\n",
        "    \"nmf_model.pkl\": \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/nmf_model.pkl\",\n",
        "    \"current_processed_menu_data.xlsx\": \"/content/drive/MyDrive/ìœ¡ì‚¬ ë¶€ì‹ ì”ë°˜ ìµœì í™”/current_processed_menu_data.xlsx\",\n",
        "}\n",
        "\n",
        "for fname, origin in file_paths.items():\n",
        "    shutil.copy(origin, os.path.join(base_dir, fname))\n",
        "\n",
        "# âœ… ì••ì¶•í•˜ê¸°\n",
        "zip_path = \"/content/offline_app_full.zip\"\n",
        "with ZipFile(zip_path, \"w\") as zipf:\n",
        "    for folder, _, files in os.walk(base_dir):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(folder, file)\n",
        "            rel_path = os.path.relpath(full_path, base_dir)\n",
        "            zipf.write(full_path, arcname=rel_path)\n",
        "\n",
        "# âœ… ë‹¤ìš´ë¡œë“œ ë§í¬ ì œê³µ\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "1reAnsC5u_eK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "7dcabf91-67e7-43cd-a5a9-b5b3e436bc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ec2145cee849>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# âœ… requirements.txt ìƒì„±\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZgRdqE2pb8g3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}